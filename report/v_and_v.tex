\chapter{Verification and Validation\ifdraft{ (Joe K./Dan/Adam) (20\%)}{}}
\label{chapter:v_and_v}

\section{Requirements and Scenarios}

\tododmz{traceability and automation; reflect back upon requirements
  and scenarios previously described}

\section{Methodology}

\subsection{Engineering Methodology}

\todoacf{Introduce context with NIST, ISO standards for methodology?}

Sound engineering practices are the foundation for building reliable
and secure software of any type. This is particularly true for
critical systems which must be trusted to perform important tasks
correctly, and where the consequences for failure threaten lives,
political system integrity, and property.

This section introduces methodologies that help reduce errors and
improve confidence in software development. We avoid discussion of
particular technologies except to illustrate our recommended
methodologies with examples in pracice.

\subsubsection{Version Control}

Version control systems (VCS) manage changes between the versions of a
project as it evolves during the course of development. Revision
control is the preferred way to share software artifacts across a
team, but all software projects, even those developed by teams as
small as one person should use VCS.

In general, a developer uses the VCS first to ``check out'' the files
comprising a project into her ``working copy''. Then, after making
changes to those files, the developer ``checks in'' or ``commits'' the
changes to the VCS. After committing, those changes are available for
other developers to integrate into their working copies.

When different sets of changes have been made by multiple developers,
the VCS can merge those changes either automatically or by using
developer input to ensure the project remains consistent. The ability
to merge changes is critical for teams of developers who work
concurrently on a single project, and is the reason that file sharing
tools like Dropbox or Google Drive are an inadequate substitute for a
VCS.

VCS is particularly important for projects that must be audited. An
entry to a project log is created every time a developer commits their
work. Any file in the project can be inspected to show its provenance,
even down to the level of which line was committed by which developer
on what date. Some VCS tools also optionally allow for commits to be
cryptographically signed, offering assurance that, for example, the
changes have been audited by a trusted authority before being
integrated into the project~\cite{chacon2014pro}.

Moving from simple file storage to VCS is a tremendous improvement for
development, but poor use of VCS can negate many of the potential
benefits. For example, the log built from the commits of developers is
of less use to auditors if the changes in each commit are not clearly
associated with a particular new feature or bug fix. Likewise if
developers commit changes in a broken state, other developers'
productivity suffers and it becomes more difficult later to isolate
which commit introduced a bug. Each VCS supports multiple workflow
practices that should be adopted in order to limit these problems and
get the most benefit from VCS
use~\cite{atlassianworkflow}\cite{pilato2008version}.

\subsubsection{Issue Tracking}

During much of the software development and maintenance process, teams
add features and fix bugs. In an issue tracking system, each new
feature, each reported bug, and other discrete development tasks are
tracked as issues from creation to implementation, review, testing,
and integration. Issues can be organized by metadata such as assignee,
project milestone, priority, and task type. Issue trackers are
essentially to-do lists with additional structure that is specialized
to support effective software engineering.

These issues and their metadata give team members a view into the
status and health of the project, and facilitates the implementation
of project management methodologies \todoacf{reference management
  section}. For example, the issue tracker may automatically require a
code review step before an issue can be resolved. Issue trackers help
teams make fewer mistakes when following best-practice software
engineering workflows.

Team members can annotate issues with comments or attach supplementary
documents, creating a record of design decisions and thought
processes. This information is invaluable when investigating future
bugs or making subsequent changes to a design, and is often lost when
such discussions take place out-of-band and lose their association
with the task that motivated them.

Most issue trackers integrate with VCS in order to associate issues
with the source code changes that were made in order to complete those
tasks. When combined with the design discussions captured in issue
comments and attachments, this further enhances the ability of the
team to reliably understand and maintain the project in the future.

Issue trackers not only benefit the project team, but also often serve
as a first line of contact for users of a system when they encounter
problems. In projects with short timelines like election systems, it
is critical to incorporate feedback into development as quickly as
possible. Giving users or front-line support staff the power to create
issues directly makes the feedback loop very small.

Keeping a public issue tracker reduces duplicated effort across both
users and developers. If a system has a flaw, that flaw will likely
become apparent to multiple users, and duplicate reports are less
likely if users can check the issue tracker for other reports of
similar problems. The development team can then coordinate an
effective response to problems. The team can triage issues by
importance and urgency, discuss potential solutions, assign developers
to implement those solutions, and finally make sure the problem is
resolved and notify the users who originally reported the problem.

\subsubsection{Testing}

Software testing practices are a key component of any software
engineering methodology. Even when parts of a system are formally
verified, testing helps provide additional assurance that the system:

\begin{itemize}
\item satisfies its requirements,
\item behaves as expected for particular inputs,
\item works correctly in diverse environments,
\item and has sufficient performance.
\end{itemize}

Testing serves the key functions of uncovering flaws quickly, and
ensuring that previously-fixed flaws do not recur in later
versions. The assurance testing provides comes from performing these
functions. It is impossible for testing to reveal all possible flaws
in any realistic program -- the number of possible inputs for any such
system is so large that an exhaustive test would effectively run
forever -- but tests that successfully reveal and prevent recurrences
of some flaws provide more assurance than an untested system.

Formal verification and testing should not be viewed as opposing
alternatives, but rather as complementary techniques that together
provide assurance in the developed software. It is not feasible to
exhaustively test every possible input and every possible path through
any non-trivial program, while formal techniques offer guarantees over
all possible inputs. However, formal techniques usually cannot scale
to provide those guarantees for entire systems, and sometimes must
make simplifying assumptions about the environment in which software
runs, or reason about a simplified model of the actual system. Since
testing can use the real system in a real environment, it can uncover
flaws that are beyond the scope or capability of the formal
techniques.

Different testing practices provide complementary types of assurance;
no single testing practice is sufficient to cover the above
list. Instead, multiple types of testing should be used in
combination on every project.

\paragraph{Unit Testing}

Unit testing exercises the smallest components (the ``units'') of a
program that are feasible to test. The granularity of unit tests
varies by programming language, but typically unit tests are small
enough to test the implementation within a single module, class, or
other per-file abstraction.

Unit tests are usually written to reflect the specification of the
unit under test. For example, a unit that implements a specification
of addition might have unit tests that check the associativity and
commutativity of the implementation. Most software specifications are
too abstract to translate directly into unit tests, so for a property
like associativity, developers must choose particular concrete values
to test. The shortcoming here is that developers might not choose the
particular values that would expose a flaw, and so the test suite
will succeed despite the presence of that flaw.

Developer intuition and understanding of the implementation increases
the likelihood that unit tests will exercise code containing a flaw,
but tests still cannot be exhaustive. Code coverage, the percentage of
lines of code exercised by a given test suite, is often used to
measure the effectiveness of unit tests. In practice, high coverage
percentages have not been shown to necessarily uncover more
flaws~\cite{inozemtseva2014coverage}, however more sophisticated
coverage measures such as branch coverage are more
promising~\cite{gligoric2013comparing}.

\tododmz{Should we talk model-based testing/JMLUnitNG here? -ACF}

\paragraph{Randomized Testing}

Manually-written tests can only exercise a small fraction of the
potential inputs to a program. When test cases can be generated
randomly, it is much cheaper to produce a large number of test cases
that can explore a larger fraction of potential inputs.

With randomized testing, developers specify a means for generating the
inputs to a test, and provide a function or ``oracle'' for evaluating
whether the test succeeded with those inputs. These test generators
can more closely mirror specifications than tests that use concrete
values, as they can instead make assertions about all possible
values.

For example, a unit test for an addition implementation might by hand
assert that $0+0=0$, $1+0=0$, and $2^{32}+0=0$, but a random test
could assert that for all integers $x$, $x+0=0$. Unless the range of
the input is very small, a random test will not provide an exhaustive
proof of the property it expresses. It can, however, easily provide
orders of magnitude more test cases than hand-written tests, and will
usually produce test cases that a developer might not think to add
intuitively. When a higher level of assurance is required, the
specification of a random test often translates directly to a logical
formula usable by formal techniques, easing the transition to a
verified system~\cite{swierstra2012xmonad}.

In the addition example above, it is straightforward to generate
random integers and check whether the result is zero. However,
randomized testing is situational, since an oracle is not always
straightforward to develop, and even the random input generator can be
quite complicated for complex input types. For example, random testing
has successfully been used to find flaws in C compilers. The
development of the C test program generator has itself been the
subject of extensive research~\cite{yang2011finding}, and the oracles
used are primarily other C compilers.

Because it is difficult to apply random testing to programs with
complex input types and no readily-available oracles, random testing
is most commonly applied to unit tests. Random testing can still be
useful in such cases by relaxing the requirements of the input
generator and oracle, and simply observing whether a program crashes
or violates internal assertions when presented with a random, possibly
malformed input. This variant of random testing is known as
``fuzzing'', and should be used as a complement to other forms of
testing.

\paragraph{Regression Testing}

In addition to unit and integration tests written to reflect the
specifications of a system, new tests should be added whenever a flaw
is uncovered and fixed. Flaws tend to recur in software for a variety
of reasons. The existence of the initial flaw implies that there is
some subtlety to that particular code, raising the baseline likelihood
of flaws. The fix applied to the code may have only fixed the flaw
under the limited set of conditions that were observed at the time,
for example in a bug report. The fix may also have depended on
assumptions about code elsewhere in the project, and the flaw can
recur once those assumptions change.

Running a regression test for every flaw in the project's history
assures us that those flaws are not present in the current software
artifacts, but for a long project, the weight of that history can make
the regression suite unfeasibly large. Many longer-term projects
therefore split their regression tests into multiple suites: a small,
quick suite to run before each code commit, a larger suite to run
every night, and sometimes a full suite that runs over weekends or
before major project milestones. Since a goal of testing is to uncover
flaws as quickly as possible, running tests less frequently is a
tradeoff, and prioritizing and minimizing the cost of testing is an
area of active research~\cite{yoo2012regression}.

\paragraph{Integration Testing}

While unit testing and regression testing detects flaws on a
per-module granularity, integration testing detects flaws in the
system as a whole. This type of testing can expose flaws in the way
multiple modules interact, measure performance of the integrated
system, reveal environmental (e.g., configuration, operating system,
network) dependencies, and simulate the end-to-end experience of the
system's users.

The most basic integration test is simply to check that the complete
system can be successfully built. Once built, integration tests
exercise substantial functionality across multiple modules, often
simulating the actions performed by a user during an interaction with
the system and checking for expected outcomes. In this sense,
integration tests are frequently the first line of validation applied
to a system.

For example, one integration test might load a ballot, make
selections, change selections, and then cast the ballot. Another
integration test might follow the same steps, but then spoil the
ballot and repeat with a new ballot. While each of these individual
steps might concern only a single module, the combination of steps
helps expose potential problems with modules interacting, for example
if a ballot successfully loads but fails to allow the ballot to be
marked correctly.

In addition to simulating end-to-end functionality, integration tests
also test the suitability of the software in its intended
environments. This is critical for systems that are intended to work
with multiple operating systems, with or without a network, or with
specialized hardware peripherals like a ballot marking device. Making
the environmental assumptions explicit in integration tests also helps
prevent flaws from arising due to unstated assumptions. For example, a
developer using the latest version of Linux may inadvertently write
software that depends on that version of Linux and fails when run on
the version of Linux used in the deployed environment. It is
counterproductive and often infeasible for developers to use the
deployed environment (it may not even be a general-purpose computer),
and so the integration tests must accurately recreate that
environment.

\subsubsection{Continuous Integration}

The expense of fixing software flaws increases over time as other
parts of a program accumulate over time around those flaws. When a
flawed feature is new, developers have not had a chance to write other
code that depends on the flawed code. Once those dependencies exist, a
fix for a single flaw can have consequences that ripple outward across
the entire project, making the fix much more expensive. The cheapest
way to fix a flaw, then, is to discover the flaw as quickly as
possible.

Continuous integration (CI) and testing facilitate this by discovering
flaws as part of a regular, automatic process that is not dependent on
due diligence of individual developers. CI interleaves the quality
control process into the development process, rather than leaving it
as a separate phase for the end of a project after development has
finished.

CI tools automatically build and test the latest version of the
software in the VCS system on a regular basis, such as every night, or
after every VCS commit. Because the software is built from the VCS, it
is important for developers to frequently commit their work to the
VCS. The VCS integration ensures the tests are always run on the
canonical version of the software, and that any discovered flaws can
be linked to a particular version in the VCS system. Instead of
potentially having to search the entire codebase for the cause of a
flaw, isolating the failing version focuses the efforts of developers
on the set of changes introduced in that version, saving time.

CI systems substantially replace manual effort and the risk of manual
mistakes when releasing software. Since the CI system is building the
project on a nightly basis, it can also post the artifacts of those
builds for users and testers to quickly adopt. When an official
release like ``Version 1.0'' is ready for release, developers can
simply run the CI system to produce the final artifact. Because the
same system is responsible for both the continuous testing and
validation of the system and the creation of the final release, it is
less likely that the final release will have flaws that would have
been caught through earlier testing.

\subsubsection{Code Review}

Code review practices involve examining the results of the software
development process to find flaws, identify potential improvements,
and increase understanding of the software throughout the engineering
team. Reviews are also an opportunity to ensure that organizational
code style standards are met, and that the code and its documentation
is clear enough that it can be effectively communicated to others
during the review. This process, like discovering flaws during
testing and investigating issue reports, feeds back into an interative
development process to improve the quality of the final product.

Code review can be a manual process at varying levels of rigor. On the
formal end, processes like Fagan inspection require a line-by-line
inspection by many developers in an extended meeting, and catch a high
percentage of flaws~\cite{fagan2002design}. On lightweight end, code
review inherently occurs during pair programming, and can take place
informally via a developer-led walkthrough or simply an email to
colleagues requesting feedback. Formal inspections are more costly
than informal reviews, but may be more suitable for projects that
require concrete audit trails for accountability. Lightweight methods,
particularly pair programming, can find similar proportions of flaws
for lower cost~\cite{tomayko2002comparison} and has other knock-on
effects such as higher developer job satisfaction and improved team
dynamics~\cite{cockburn2000costs}.

Automated tools complement any form of manual code review. Lightweight
static analysis tools and code ``lint'' tools can help developers
avoid common coding mistakes and adhere to organizational style
standards. Very lightweight static tools can be run by individual
developers before committing their work to the VCS, and longer-running
analyses can be part of the continuous integration and testing
process. Such analyses are not substitutes for formal verification,
however, as they typically are meant to discover small-scale defects
and help developers avoid common pitfalls rather than proving overall
properties about the correctness of a system.

\subsubsection{Release Management \& Lifecycle}

\subsubsection{First-Class Documentation}

\subsubsection{Automation}

\section{Technologies}
\section{Interpreting Results}
