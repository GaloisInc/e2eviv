\chapter{Rigorous Software Engineering\ifdraft{ (Joe K./Dan/Adam) (40\%)}{}}
\label{cha:rigor-softw-engin}

Sound engineering practices are the foundation for building reliable
and secure software of any type. This is particularly true for
critical systems which must be trusted to perform important tasks
correctly, and where the consequences for failure threaten lives,
political system integrity, and property.

This section introduces methodologies that help reduce errors and
improve confidence in software development. We avoid discussion of
particular technologies except to illustrate our recommended
methodologies with examples in pracice.

\section{Informal System Specifications}

\subsection{Domain Models}

\subsection{Requirements and Scenarios}

\tododmz{traceability and automation; reflect back upon requirements
  and scenarios previously described}

\subsection{Concept Specifications}

\subsection{Utility of Informal Specifications}

\section{Formal System Specifications}

\subsection{Architecture Specifications}

\subsection{Concept Specifications}

\subsection{Source Code Specifications}

\subsection{Protocol Specifications}

\subsection{Utility of Formal Specifications}

\section{Methodology}

\subsection{Engineering Methodology}

\todoacf{Introduce context with NIST, ISO standards for methodology?}

\todoacf{Lay out the characteristics we want more explicitly, eg,
  freedom from flaws, repetability, correctness}

\subsubsection{Version Control}

Version control systems (VCS) manage changes between the versions of a
project as it evolves during the course of development. Revision
control is the preferred way to share software artifacts across a
team, but all software projects, even those developed by teams as
small as one person should use VCS.

In general, a developer uses the VCS first to ``check out'' the files
comprising a project into her ``working copy''. Then, after making
changes to those files, the developer ``checks in'' or ``commits'' the
changes to the VCS. After committing, those changes are available for
other developers to integrate into their working copies.

When different sets of changes have been made by multiple developers,
the VCS can merge those changes either automatically or by using
developer input to ensure the project remains consistent. The ability
to merge changes is critical for teams of developers who work
concurrently on a single project, and is the reason that file sharing
tools like Dropbox or Google Drive are an inadequate substitute for a
VCS.

VCS is particularly important for projects that must be audited. An
entry to a project log is created every time a developer commits their
work. Any file in the project can be inspected to show its provenance,
even down to the level of which line was committed by which developer
on what date. Some VCS tools also optionally allow for commits to be
cryptographically signed, offering assurance that, for example, the
changes have been audited by a trusted authority before being
integrated into the project~\cite{chacon2014pro}.

Moving from simple file storage to VCS is a tremendous improvement for
development, but poor use of VCS can negate many of the potential
benefits. For example, the log built from the commits of developers is
of less use to auditors if the changes in each commit are not clearly
associated with a particular new feature or bug fix. Likewise if
developers commit changes in a broken state, other developers'
productivity suffers and it becomes more difficult later to isolate
which commit introduced a bug. Each VCS supports multiple workflow
practices that should be adopted in order to limit these problems and
get the most benefit from VCS
use~\cite{atlassianworkflow}\cite{pilato2008version}.

\subsubsection{Issue Tracking}

During much of the software development and maintenance process, teams
add features and fix bugs. In an issue tracking system, each new
feature, each reported bug, and other discrete development tasks are
tracked as issues from creation to implementation, review, testing,
and integration. Issues can be organized by metadata such as assignee,
project milestone, priority, and task type. Issue trackers are
essentially to-do lists with additional structure that is specialized
to support effective software engineering.

These issues and their metadata give team members a view into the
status and health of the project, and facilitates the implementation
of project management methodologies \todoacf{reference management
  section}. For example, the issue tracker may automatically require a
code review step before an issue can be resolved. Issue trackers help
teams make fewer mistakes when following best-practice software
engineering workflows.

Team members can annotate issues with comments or attach supplementary
documents, creating a record of design decisions and thought
processes. This information is invaluable when investigating future
bugs or making subsequent changes to a design, and is often lost when
such discussions take place out-of-band and lose their association
with the task that motivated them.

Most issue trackers integrate with VCS in order to associate issues
with the source code changes that were made in order to complete those
tasks. When combined with the design discussions captured in issue
comments and attachments, this further enhances the ability of the
team to reliably understand and maintain the project in the future.

Issue trackers not only benefit the project team, but also often serve
as a first line of contact for users of a system when they encounter
problems. In projects with short timelines like election systems, it
is critical to incorporate feedback into development as quickly as
possible. Giving users or front-line support staff the power to create
issues directly makes the feedback loop very small.

Keeping a public issue tracker reduces duplicated effort across both
users and developers. If a system has a flaw, that flaw will likely
become apparent to multiple users, and duplicate reports are less
likely if users can check the issue tracker for other reports of
similar problems. The development team can then coordinate an
effective response to problems. The team can triage issues by
importance and urgency, discuss potential solutions, assign developers
to implement those solutions, and finally make sure the problem is
resolved and notify the users who originally reported the problem.

\subsubsection{Testing}

Software testing practices are a key component of any software
engineering methodology. Even when parts of a system are formally
verified, testing helps provide additional assurance that the system:

\begin{itemize}
\item satisfies its requirements,
\item behaves as expected for particular inputs,
\item works correctly in diverse environments,
\item and has sufficient performance.
\end{itemize}

Testing serves the key functions of uncovering flaws quickly, and
ensuring that previously-fixed flaws do not recur in later
versions. The assurance testing provides comes from performing these
functions. It is impossible for testing to reveal all possible flaws
in any realistic program---the number of possible inputs for any such
system is so large that an exhaustive test would effectively run
forever---but tests that successfully reveal and prevent recurrences
of some flaws provide more assurance than an untested system.

Formal verification and testing should not be viewed as opposing
alternatives, but rather as complementary techniques that together
provide assurance in the developed software. It is not feasible to
exhaustively test every possible input and every possible path through
any non-trivial program, while formal techniques offer guarantees over
all possible inputs. However, formal techniques usually cannot scale
to provide those guarantees for entire systems, and sometimes must
make simplifying assumptions about the environment in which software
runs, or reason about a simplified model of the actual system. Since
testing can use the real system in a real environment, it can uncover
flaws that are beyond the scope or capability of the formal
techniques.

Different testing practices provide complementary types of assurance;
no single testing practice is sufficient to cover the above
list. Instead, multiple types of testing should be used in
combination on every project.

\paragraph{Unit Testing}

Unit testing exercises the smallest components (the ``units'') of a
program that are feasible to test. The granularity of unit tests
varies by programming language, but typically unit tests are small
enough to test the implementation within a single module, class, or
other per-file abstraction.

Unit tests are usually written to reflect the specification of the
unit under test. For example, a unit that implements a specification
of addition might have unit tests that check the associativity and
commutativity of the implementation. Most software specifications are
too abstract to translate directly into unit tests, so for a property
like associativity, developers must choose particular concrete values
to test. The shortcoming here is that developers might not choose the
particular values that would expose a flaw, and so the test suite
will succeed despite the presence of that flaw.

Developer intuition and understanding of the implementation increases
the likelihood that unit tests will exercise code containing a flaw,
but tests still cannot be exhaustive. Code coverage, the percentage of
lines of code exercised by a given test suite, is often used to
measure the effectiveness of unit tests. In practice, high coverage
percentages have not been shown to necessarily uncover more
flaws~\cite{inozemtseva2014coverage}, however more sophisticated
coverage measures such as branch coverage are more
promising~\cite{gligoric2013comparing}.

\paragraph{Randomized Testing}

Manually-written tests can only exercise a small fraction of the
potential inputs to a program. When test cases can be generated
randomly, it is much cheaper to produce a large number of test cases
that can explore a larger fraction of potential inputs.

With randomized testing, developers specify a means for generating the
inputs to a test, and provide a function or ``oracle'' for evaluating
whether the test succeeded with those inputs. These test generators
can more closely mirror specifications than tests that use concrete
values, as they can instead make assertions about all possible
values.

For example, a unit test for an addition implementation might by hand
assert that $0+0=0$, $1+0=0$, and $2^{32}+0=0$, but a random test
could assert that for all integers $x$, $x+0=0$. Unless the range of
the input is very small, a random test will not provide an exhaustive
proof of the property it expresses. It can, however, easily provide
orders of magnitude more test cases than hand-written tests, and will
usually produce test cases that a developer might not think to add
intuitively. When a higher level of assurance is required, the
specification of a random test often translates directly to a logical
formula usable by formal techniques, easing the transition to a
verified system~\cite{swierstra2012xmonad}.

In the addition example above, it is straightforward to generate
random integers and check whether the result is zero. However,
randomized testing is situational, since an oracle is not always
straightforward to develop, and even the random input generator can be
quite complicated for complex input types. For example, random testing
has successfully been used to find flaws in C compilers. The
development of the C test program generator has itself been the
subject of extensive research~\cite{yang2011finding}, and the oracles
used are primarily other C compilers.

Because it is difficult to apply random testing to programs with
complex input types and no readily-available oracles, random testing
is most commonly applied to unit tests. Random testing can still be
useful in such cases by relaxing the requirements of the input
generator and oracle, and simply observing whether a program crashes
or violates internal assertions when presented with a random, possibly
malformed input. This variant of random testing is known as
``fuzzing'', and should be used as a complement to other forms of
testing.

\paragraph{Model-Based Testing}

Another method for automatically generating test cases is model-based
testing, where formal models of the program's behavior can be used as
test oracles, as guides for choosing test data, or both.

Formal models of program behavior can be used as test oracles by
compiling them into the software as \emph{runtime assertion
  checks}. For example, assume the specification for some function $f$
guarantees that some condition $x$ is true when it returns. If $x$ is
ever false when $f$ returns, $f$ does not satisfy its
specification. Any good set of unit tests for $f$ should detect this
divergence, and it would certainly be straightforward to write (by
hand) a test oracle that checked the value of $x$ after executing
$f$. However, in many cases $f$'s specification can be
\emph{automatically} transformed into a set of runtime assertions,
such that (in this case) an error is always raised if $x$ is false
when $f$ returns.  Compilers that perform such transformations are
available for several programming languages and corresponding
specification languages; some widely-used examples are Java and the
Java Modeling Language (JML), C\# and Code Contracts, C and the
Executable ANSI/ISO C Specification Language (E-ACSL), and Eiffel
(with its integrated specification language).

The set of runtime assertion checks derived from a formal
specification of a module effectively becomes a test oracle for that
module; in general, more precise specifications lead to better test
oracles. For a function $f$ with such an oracle, each possible input
to $f$ defines a test, and each test is run by simply calling $f$ with
that input. A test passes if all the runtime assertion checks pass and
fails if any runtime assertion check fails. If the specification of
$f$ restricts the set of possible inputs, tests that supply invalid
input are effectively meaningless and their output is ignored.

Of course, it is impractical to call $f$ with every possible input;
however, strategies such as randomized input data generation
(discussed above) and ``interesting'' input data generation (for
example, ensuring that all boundary conditions are tested for data
types with boundaries, such as numeric types) can be used to cover the
functionality of $f$ with a reasonable number of tests. Multiple test
frameworks use these techniques to automatically generate and run
model-based tests, including the JMLUnitNG tool for
Java/JML~\cite{ZimmermanNagmoti10} (which integrates with the
TestNG~\cite{TestNG} automated testing framework) and the AutoTest
tool for Eiffel~\cite{AutoTest10}.

Formal models of program behavior can also be used as guides for
choosing test data; for example, AutoTest uses constraint solving
techniques on programs and their specifications to ensure that test
data satisfies input constraints on the functions under test.
Techniques such as symbolic execution---analysis of the program to
determine what inputs cause what execution paths to be taken---can
also be used to choose test data, with the goal of achieving maximal
coverage using a minimal set of test cases. The PEX testing tool for
.NET~\cite{PEX08} uses this technique.

\paragraph{Regression Testing}

In addition to unit and integration tests written to reflect the
specifications of a system, new tests should be added whenever a flaw
is uncovered and fixed. Flaws tend to recur in software for a variety
of reasons. The existence of the initial flaw implies that there is
some subtlety to that particular code, raising the baseline likelihood
of flaws. The fix applied to the code may have only fixed the flaw
under the limited set of conditions that were observed at the time,
for example in a bug report. The fix may also have depended on
assumptions about code elsewhere in the project, and the flaw can
recur once those assumptions change.

Running a regression test for every flaw in the project's history
assures us that those flaws are not present in the current software
artifacts, but for a long project, the weight of that history can make
the regression suite unfeasibly large. Many longer-term projects
therefore split their regression tests into multiple suites: a small,
quick suite to run before each code commit, a larger suite to run
every night, and sometimes a full suite that runs over weekends or
before major project milestones. Since a goal of testing is to uncover
flaws as quickly as possible, running tests less frequently is a
tradeoff, and prioritizing and minimizing the cost of testing is an
area of active research~\cite{yoo2012regression}.

\paragraph{Integration Testing}

While unit testing and regression testing detects flaws on a
per-module granularity, integration testing detects flaws in the
system as a whole. This type of testing can expose flaws in the way
multiple modules interact, measure performance of the integrated
system, reveal environmental (e.g., configuration, operating system,
network) dependencies, and simulate the end-to-end experience of the
system's users.

The most basic integration test is simply to check that the complete
system can be successfully built. Once built, integration tests
exercise substantial functionality across multiple modules, often
simulating the actions performed by a user during an interaction with
the system and checking for expected outcomes. In this sense,
integration tests are frequently the first line of validation applied
to a system.

For example, one integration test might load a ballot, make
selections, change selections, and then cast the ballot. Another
integration test might follow the same steps, but then spoil the
ballot and repeat with a new ballot. While each of these individual
steps might concern only a single module, the combination of steps
helps expose potential problems with modules interacting, for example
if a ballot successfully loads but fails to allow the ballot to be
marked correctly.

In addition to simulating end-to-end functionality, integration tests
also test the suitability of the software in its intended
environments. This is critical for systems that are intended to work
with multiple operating systems, with or without a network, or with
specialized hardware peripherals like a ballot marking device. Making
the environmental assumptions explicit in integration tests also helps
prevent flaws from arising due to unstated assumptions. For example, a
developer using the latest version of Linux may inadvertently write
software that depends on that version of Linux and fails when run on
the version of Linux used in the deployed environment. It is
counterproductive and often infeasible for developers to use the
deployed environment (it may not even be a general-purpose computer),
and so the integration tests must accurately recreate that
environment.

\subsubsection{Continuous Integration}

The expense of fixing software flaws increases over time as other
parts of a program accumulate over time around those flaws. When a
flawed feature is new, developers have not had a chance to write other
code that depends on the flawed code. Once those dependencies exist, a
fix for a single flaw can have consequences that ripple outward across
the entire project, making the fix much more expensive. The cheapest
way to fix a flaw, then, is to discover the flaw as quickly as
possible.

Continuous integration (CI) and testing facilitate this by discovering
flaws as part of a regular, automatic process that is not dependent on
due diligence of individual developers. CI interleaves the quality
control process into the development process, rather than leaving it
as a separate phase for the end of a project after development has
finished.

CI tools automatically build and test the latest version of the
software in the VCS system on a regular basis, such as every night, or
after every VCS commit. Because the software is built from the VCS, it
is important for developers to frequently commit their work to the
VCS. The VCS integration ensures the tests are always run on the
canonical version of the software, and that any discovered flaws can
be linked to a particular version in the VCS system. Instead of
potentially having to search the entire codebase for the cause of a
flaw, isolating the failing version focuses the efforts of developers
on the set of changes introduced in that version, saving time.

CI systems substantially replace manual effort and the risk of manual
mistakes when releasing software. Since the CI system is building the
project on a nightly basis, it can also post the artifacts of those
builds for users and testers to quickly adopt. When an official
release like ``Version 1.0'' is ready for release, developers can
simply run the CI system to produce the final artifact. Because the
same system is responsible for both the continuous testing and
validation of the system and the creation of the final release, it is
less likely that the final release will have flaws that would have
been caught through earlier testing.

\subsubsection{Code Review}

Code review practices involve examining the results of the software
development process to find flaws, identify potential improvements,
and increase understanding of the software throughout the engineering
team. Reviews are also an opportunity to ensure that organizational
code style standards are met, and that the code and its documentation
is clear enough that it can be effectively communicated to others
during the review. This process, like discovering flaws during
testing and investigating issue reports, feeds back into an interative
development process to improve the quality of the final product.

Code review can be a manual process at varying levels of rigor. On the
formal end, processes like Fagan inspection require a line-by-line
inspection by many developers in an extended meeting, and catch a high
percentage of flaws~\cite{fagan2002design}. On lightweight end, code
review inherently occurs during pair programming, and can take place
informally via a developer-led walkthrough or simply an email to
colleagues requesting feedback. Formal inspections are more costly
than informal reviews, but may be more suitable for projects that
require concrete audit trails for accountability. Lightweight methods,
particularly pair programming, can find similar proportions of flaws
for lower cost~\cite{tomayko2002comparison} and has other knock-on
effects such as higher developer job satisfaction and improved team
dynamics~\cite{cockburn2000costs}.

Automated tools complement any form of manual code review. Lightweight
static analysis tools and code ``lint'' tools can help developers
avoid common coding mistakes and adhere to organizational style
standards. Very lightweight static tools can be run by individual
developers before committing their work to the VCS, and longer-running
analyses can be part of the continuous integration and testing
process. Such analyses are not substitutes for formal verification,
however, as they typically are meant to discover small-scale defects
and help developers avoid common pitfalls rather than proving overall
properties about the correctness of a system.

\subsubsection{Release Management \& Lifecycle}

Release management is the process through which software moves from
implementation through testing, validation, and verification into a
finished product that can be used in its intended environment. Release
management is primarily focused on the smooth integration of the
different aspects of the project, and on adhering to practices that
make releases repeatable, reliable, and auditable.

Release management and VCS workflows are tightly connected. For
example, release managent would be responsible for creating a new
release branch in the Git Flow model~\cite{atlassianworkflow},
imposing a feature freeze (no new features, only bug fixes) on that
branch, and eventually tagging that branch upon release and merging it
back into the main development branch.

For a project that delivers software as a service on a web server,
release management would be responsible for deploying the software to
production servers. For software delivered as a binary download or CD,
release management would be responsible for cryptographically signing
and distributing the binary. In both of these cases, the release
manager serves as the final line of quality assurance before the
software is used in its intended environment, and so she must be
fluent enough with all aspects of the project and its processes to
release software only once the processes have been faithfully
executed.

\subsubsection{Testable Documentation}

Documentation of the design, implementation, and use of a software
system is a standard requirement in software engineering
methodologies. However when a system is under development and rapidly
changing, documentation can lag behind and fall out of step with the
latest version of the software, leading to errors and confusion.

Where possible, documentation should be machine-testable (or even
machine-generated) and integrated with the VCS rather than being a set
of static resources maintained independently of the software
itself. Much as testing gives the advantage of early software flaw
detection, testable and generated documentation is less likely to
become inconsistent with the software it describes.

The form of testable documentation varies depending on the granularity
of the documentation and the underlying technologies used by the
project. For example, Business Object Notation
(BON)~\cite{walden1995seamless} can be used as analyzable
documentation at the specification, design, and architecture level.

At the level of code modules and interfaces, documentation should be
concretely executable like the ``doctest'' features available for
programming languages like Python and
Haskell~\cite{python3doctest}. Documentation in this style contains
short examples that illustrate the expected use of a system and its
expected response, for example in Python:

\begin{lstlisting}[language=Python]
"""
This is the fibonacci module. It provides the function fib which
returns the nth fibonacci number, where n >= 0.

>>> fib(0)
0
>>> fib(10)
55
>>> fib(-1)
Traceback (most recent call last):
    ...
ValueError: n must be >= 0
"""
\end{lstlisting}

These executable tests should supplement, not replace traditional
prose documentation. Since they amount to a form of unit test, they
suffer from the same limitations. They usually only exercise a handful
of concrete values, and cannot test non-functional properties like
expected performance or thread safety.

\subsubsection{Reproducibility \& Automation}

A team can implement many of the processes in this section
manually. Tests can be run by hand on developers' machines, code can
be sent out for review by email, issues can be tracked in mailing list
posts, and a release manager can build and package release artifacts
by hand for each supported platform. Each time a step in a process
must be manually performed, the probability of human error increases,
and reproducing steps for later quality assurance and troubleshooting
becomes harder.

A manual operator might skip a step or perform a step out of order,
for example running the test suite before integrating the latest
changes from the VCS. The operator might also introduce new steps that
seem necessary and obvious, but unless recorded will make it very
difficult to reproduce or audit the process in the future. Finally,
manual execution of a process, even if done correctly, takes much
longer than automated execution.

To prevent errors, improve reproducibility, and make development more
efficient, processes should be automated as much as possible. The
methodologies described in this section all support automation and
reproducibility or can themselves be automated to a degree, but some
play key roles:

\paragraph{Version Control}

The version control system is a linchpin of automation on a
project. The versions it manages are the starting point for automated
and reproducible continuous integration, testing, and software
releases. The VCS can itself trigger automated processes, for example
it could trigger a run of the test suite after every commit.

Any automated process should be run in the context of a particular VCS
version, and any artifacts produced by these processes should refer to
this version. For example, if software has a built-in bug reporting
feature, those reports should automatically include the version at the
time the software was built. This allows engineers to easily reproduce
the exact circumstances where the user discovered a flaw.

Version control can only improve reproducibility when all of the
relevant inputs to a process are managed in the VCS. For example, a
software build process that depends on a configuration file in the
user's home directory would not be reproducible on a different
computer without that home directory.

\paragraph{Testing}

Manual testing can play an important role when evaluating a system,
but any realistic system requires more tests than are feasible to
perform manually. Even if the contents of a test suite are automatic,
if that test suite is only run manually it will often be skipped,
particularly when it takes a long time to run. Automating both the
tests themselves and the running of those tests ensures that they will
be run on a consistent basis, and that the results will be
reproducible and traceable to a particular version.

\paragraph{Continuous Integration}

Continuous integration is another linchpin of project
automation. Since CI tools are designed to automatically run on a
regular basis and offer integration with the VCS, other processes are
usually automated by using these tools. For example, after building
the integrated software, a CI tool should run the test suite and
archive the built artifacts for subsequent release management.

\paragraph{Release Management}

No process has as many moving parts or cross-project concerns as
release management, making manual release management extremely
error-prone. The entire process from checking out a version from the
VCS to deploying the final release artifacts should be as automatic as
possible. The manual intervention should amount to simply deciding
which version to release, and checking before the final release that
the automated process performed as expected.

Because automation is inexpensive when using continuous integration,
it is a good practice to have CI tools perform parts of the release
management process on a regular basis, even when software is not ready
for a release. If the process of producing a release is the same as
performing an ordinary nightly build and test, it is less likely that
problems will arise only at the release stage when it is much more
costly to address those problems.

\section{Technologies}
\label{sec:technologies}

\section{Evidence-based Elections Technology}

\subsection{Measuring and Assessing Quality}

\subsection{Interpreting Evidence for the Non-expert}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "report"
%%% End:
