\chapter{Rigorous Software Engineering\ifdraft{ (Joe K./Dan/Adam) (40\%)}{}}
\label{cha:rigor-softw-engin}

Sound engineering practices are the foundation for building reliable
and secure software of any type. This is particularly true for
critical systems that must be trusted to perform important tasks
correctly, and where the consequences for failure threaten lives,
property, or political system integrity. In this chapter, we introduce
rigorous software engineering techniques related to specification,
implementation, testing, maintenance, and project management that help
reduce errors and improve confidence in software development. We also
provide recommendations for specific technologies to use in the
construction of critical systems such as E2E-VIV systems.

\tododmz{A big picture of everything is missing here.  An illustration
  is relevant.  I'm thinking something inspired by the BON book
  appendix, augmented with formal verification and more synthesis.}
\tododmz{Take a full pass through to eliminate passive voice.}

\section{Informal System Specifications}

An \emph{informal system specification} is a human-readable
description of the system's purpose, functionality, and high-level
design. Informal specifications should be understandable not only to
the system's developers, but also to other stakeholders: clients for
whom the system is being developed, users of the system,
administrative staff who maintain the system, and auditors who
evaluate the system.

A complete informal system specification consists of a domain model, a
set of requirements and scenarios, and a set of concept
specifications.

\subsection{Domain Models}

A \emph{domain model} of a system identifies the various concepts
(also called \emph{classes} or \emph{classifiers} in some domain
modeling techniques) in the system, their attributes and roles, and
the client and inheritance relationships among them. Domain models can
be expressed in various ways, both textual and graphical. Regardless
of their form, they are essentially lists of concepts with associated
brief human-readable descriptions of themselves and their
relationships to other concepts. A good domain model provides a shared
vocabulary and gives a big picture view of the concepts involved in
the system upon which all the stakeholders can agree.

For example, the domain model for E2E-VIV systems (see
\autoref{appendix:domain_model}) includes, among many others, the
concepts ``election'', ``contest'', ``ballot question'', and
``choice''. It describes an election as ``a formal process of
selecting choices in one or more contests'', and says that a ballot
question is a specific type of contest, namely ``a decision among two
or more courses of action''. The relationship between a ballot
question and a contest is an inheritance relationship: a ballot
question \emph{is a} specific type of contest, with a certain set of
characteristics. One relationship among elections, choices, and
contests is that an election \emph{has}, or \emph{is a client of}, at
least one choice and at least one contest, because the description of
an election explicitly refers to choices and contests.

Importantly, while a good domain model comprehensively identifies and
defines the concepts and relationships in a system, it does not
otherwise constrain the realization of those concepts and
relationships or the actual behavior of the system. The description of
``ballot question'' does not specify \emph{how} a decision among two
or more courses of action is made, and the description of ``election''
does not specify \emph{how} choices in contests are selected. Such
constraints on the realization and behavior of the system are
described in general terms in requirements, scenarios, and informal
concept specifications, and are precisely expressed in formal
specifications.

\subsection{Requirements and Scenarios}

A system's \emph{requirements} are, essentially, the statements that
must be true of the system's implementation when it is
complete. Requirements are typically phrased as simple
natural-language sentences, each of which expresses a single testable
property of the system. ``The e-voting system shall maintain the
privacy of individuals.'' and ``All voter authentication secrets must
be changed at least once in every election cycle.'' are examples of
E2E-VIV requirements; the full set of requirements is described in
\autoref{chapter:required_properties}, and the informal requirements
document itself is in \autoref{appendix:bon_requirements}.

There are several different kinds of requirement, but they can be
broadly divided into two basic types: \emph{functional} and
\emph{non-functional}. Functional requirements are those that specify
how the system must behave and what outputs it must generate given
certain sets of inputs under certain conditions. For example, one
functional requirement of E2E-VIV systems is that only eligible voters
may cast ballots. Non-functional requirements are those that specify
overall characteristics of the system, such as a lower bound on its
availability or an upper bound on its cost. For example, one
non-functional requirement of E2E-VIV systems is that they must be
Open Source.

\emph{Scenarios} are descriptions of interactions among the entities
in the system, or between the system and its environment. A scenario
is typically expressed as a short paragraph describing the actions of
the entities involved, though numbered lists (to indicate a sequence
of operations) or communication diagrams (to indicate the flow of data
during a scenario) are also reasonable representations. A scenario may
also be expressed as a collection of requirements that, together,
describe the behavior of a part of the system under certain
circumstances.

It is important that requirements and scenarios describe not only the
\emph{ideal} behavior of the system, but also how the system deals
with situations that are less than ideal. System failures,
communication disruptions, data corruption, and malicious attacks of
various kinds should all be addressed in a system's requirements and
scenarios. It is not necessary to specify these at a level of detail
that provides explicit sequences of steps to recover from any given
failure or attack; rather, they can be addressed in general terms. For
example, one requirement of E2E-VIV systems that addresses system
failures is ``If service goes down for any reason other than regional
natural disaster or malicious attack, service must be restored in no
more than 10 minutes''.

As the system is built, \emph{traceability} of the requirements and
scenarios is critical. It must be possible to identify, for every
requirement and scenario, the related parts of the formal
specification and implementation, and the links among informal
specification, formal specification, and implementation need to be
kept current when any of them change. Moreover, it must be possible to
identify one or more system tests demonstrating that the system
implementation adequately addresses each requirement or scenario. This
should preferably be done automatically as part of testing and
continuous integration processes, in a way that makes it clear to
developers which, if any, requirements and scenarios remain to be
addressed.

\subsection{Concept Specifications}

A \emph{concept specification} is an informal behavioral description
of a concept in the system's domain model. Concept specifications help
to clarify what the roles and responsibilities of the concepts in the
model are with respect to the overall functionality of the system. A
typical concept specification is a set of English sentences called
\emph{queries}, \emph{commands} and \emph{constraints}, describing
respectively what information the concept possesses, what the concept
can do, and what limitations exist on its behavior. 

A query is phrased as a simple question, such as ``How many votes have
been cast?'' or ``Is this voter eligible to vote in this contest?'',
that the concept must be able to answer. Queries provide an informal
description of the data encapsulated in the concept, because the
concept must contain all the data necessary to answer any of the
queries that may be posed to it.

A command is phrased as an exclamation, such as ``Cast this vote!'' or
``Log this user out!'', describing an action that the concept must
take. This provides an informal description of the behavior
encapsulated in the concept, because the concept must be able to take
all (and only) the actions described by its commands.

Finally, a constraint is phrased as a declaration, such as ``A voter
must be eligible for this election to cast a vote.'' or ``Only users
that are logged in may be logged out.'', describing the circumstances
under which queries and commands may be issued. This provides an
informal description of the conditions under which queries and
commands may be issued to a concept. 

An informal concept specification has two main purposes: first, it
clarifies the roles, responsibilities and relationships of a concept
in the system. The process of creating informal concept specifications
may lead to the realization that some necessary concepts are missing
from the domain analysis, that some single concepts are actually
multiple related concepts that need to be described separately, or
that multiple concepts that were thought to be distinct are actually
aspects of the same concept. Second, it serves as a ``template'' for
developers implementing the concept, and---with appropriate tool
support for the formal specification and implementation
languages---can be used to automatically generate initial formal
specifications and implementation frameworks.

\section{Formal System Specifications}

A \emph{formal system specification} is a machine-readable,
mathematical description of the system's functionality and design. It
is a refinement of an informal system specification, and there must be
a demonstrable and traceable correspondence between the informal and
formal system specifications. Formal specifications are meant to be
used both by the system's developers and by the various tools employed
during the development and testing process to validate the system
against its specification. They may also be used to automatically
generate partial or full implementations of system components, create
and run test suites, and generate part of the system documentation.

A complete formal system specification consists of architecture,
concept, source code, and protocol specifications; some of these, such
as the architecture specification, may be minimal for simple
systems.

\subsection{Architecture Specifications}

An \emph{architecture specification} is a precise description of the
system's architecture, formalizing both the relationships among the
concepts in the system and the relationships between these concepts
and the physical implementation of the system. 

\todokiniry{A bit of further discussion, without talking about tools.}

\subsection{Concept Specifications}

A \emph{formal concept specification} is a formal behavioral
description of a concept in the system's domain model. Formal concept
specifications are refinements of informal concept specifications, as
described in the previous section. It is possible for multiple
informal concept specifications to refine to the same formal concept
specification; for example, two informal concept specifications
``list'' and ``queue'' might both refine to the formal concept
specification ``linear data structure''.

Like informal concept specifications, formal concept specifications
have queries and commands; in formal concept specifications, these are
expressed as typed interfaces to the concept called
\emph{features}. For example, a formal specification of the
``election'' concept might refine the informal query ``How many votes
have been cast?'' with a feature of type \texttt{integer} called
\texttt{number\_of\_votes\_cast}, and the informal command ``Cast this
vote!'' with a feature of type \texttt{vote -> void} (that is, taking
a vote as a parameter and returning no result) called
\texttt{cast\_vote}. It is possible for a single query or command to
refine to multiple features of different types; for example, a ``Log
in!''  command might be implemented as two features, one taking
username and password parameters and the other taking an
authentication token parameter.

Features may be restricted, accessible only to a given concept or
group of concepts, or unrestricted, accessible to the entire
system. It is possible for a concept to have restricted features that
are not direct refinements of its informal queries and commands. For
example, the ``election'' concept might have restricted features
representing the entire database of candidates for office and ways to
update that database, but only allow unrestricted access to the
database via a query (refined from the informal specification)
requesting the candidates for a particular race.

Formal concept specifications also have constraints, expressed as
\emph{preconditions} and \emph{postconditions} on features or as
\emph{invariants}, as appropriate. Preconditions, postconditions and
invariants are part of the Design by Contract development
technique~\cite{OOSC}: a precondition is a predicate that must be true
in order to invoke a feature; a postcondition is a predicate that must
be true after a feature has been invoked; and an invariant is a
predicate that must be true at all (observable) times. For example, an
informal constraint that the number of votes cast can never be
negative could be expressed as a postcondition \texttt{Result > 0} on
the \texttt{number\_of\_votes\_cast} feature (guaranteeing that the
result of invoking the feature is always non-negative), or could be
expressed as an invariant \texttt{number\_of\_votes\_cast > 0} on the
``election'' concept as a whole.

In addition to refinements of queries, commands and constraints, each
formal concept specification also contains refinements of inheritance
and client relationships described in the domain model. Inheritance
relationships are directly stated in formal concept specifications,
and client relationships are implicitly stated through the types
assigned to the concept's features.

A formal concept specification, like an informal one, has two main
purposes. First, it expresses some of the requirements of the system
in precise mathematical terms that can be checked for various
desirable properties, such as logical consistency, before
implementation begins; this allows for early detection of a class of
possible problems with the requirements or their realizations. Second,
it can be used with automated code generation tools to create a
significant amount of the implementation and source code specification
automatically, in a provably-correct and traceable fashion.

\subsection{Source Code Specifications}

A \emph{source code specification} is an annotation integrated into,
or otherwise associated with, a piece of source code that makes formal
statements about the code's behavior. Source code specifications are
typically direct refinements of the preconditions, postconditions and
invariants from formal concept specifications, and often are the same
mathematical statements rendered in a different syntax. However,
source code specifications may also formalize aspects of the system
that are not dealt with in the formal concept specifications, such as
memory and CPU usage limits, algorithmic efficiency, fine-grain
concurrency properties, and cross-concept properties that cannot be
expressed (or are difficult to express) in individual concept
specifications, but are otherwise expressed in, e.g., non-functional
requirements.

Source code specifications are generally written in a language that is
closely related to the implementation language, and in some cases are
written in the implementation language itself. Thus, the choice of
implementation language has a significant effect on what source code
specifications can be written and how much effort it takes to write
them. With appropriate tool support, which is available for several
implementation languages of various styles, many source code
specifications can be generated automatically from formal concept
specifications. Additionally, some can be inferred by analyzing parts
of the implementation.

Source code specifications enable extended static checking
(ESC)~\cite{ESC98} tools to verify, using automated theorem proving
techniques, that implementations satisfy their formal specifications
without actually executing the code. This verification is
\emph{modular}, meaning that individual components of the
implementation are verified in isolation. For example, when verifying
the postcondition of a feature $X$ the verifier makes a number of
assumptions: first, that $X$'s precondition is satisfied when $X$ is
invoked; second, that all invariants applicable to $X$ are satisfied
when $X$ is invoked; and third, that all features invoked by $X$
behave correctly with respect to their specifications. This modularity
allows static verification to be performed continuously during system
development in an efficient, incremental fashion, providing assurance
that the completed parts of the implementation are correct and
highlighting the parts of the implementation that do not yet satisfy
their specifications.

Source code specifications can provide significant benefits at runtime
as well. Runtime assertion checking compilers can use the
specifications to generate executable code that continuously checks
for runtime violations of the specification and flags errors when such
violations occur. Automated test generators can use the specifications
to generate high-coverage unit test suites to exercise the
implementation. Runtime assertion checking and automated test
generation can significantly increase the reliability of the finished
system, and in most cases require minimal developer effort after the
specifications are written.

\subsection{Protocol Specifications}

A \emph{protocol specification} is a formal description of an
information exchange among multiple parts of a system. Each distinct
type of information exchange in the system---such as registering a
candidate or casting a vote---has an associated protocol that must be
followed. We consider only \emph{application-level protocols} that
specify what type of information is exchanged, how it is encoded, how
(and whether) it is encrypted or digitally signed, and the sequences
of interactions the involved parties perform. We assume the existence
of well-specified lower-level protocols that enable information
transmission, such as the transport protocols used to encode
information into Internet Protocol packets and the physical protocols
used to convert those packets into electrical or optical signals and
send them to remote destinations.

Application-level protocols are described, typically in terms of
communicating finite-state machines, using one of several languages
specifically devised for protocol specification. Automated tools
process these descriptions to verify that the protocols have, or
demonstrate that they do not have, various properties. These include
both security properties, such as whether an adversary can gain access
to data that is supposed to be secret or modify data without being
detected, and non-security properties, such as whether the protocol
always terminates in an acceptable state. Protocol specification
languages and tools are typically designed to specify and verify
cryptographic protocols, but can also be used to specify and verify
insecure communication protocols by simply ignoring security
properties; in the case of an E2E-VIV system, the vast majority of the
application-level protocols are cryptographic protocols and require
security property verification.

The choice of protocol specification language is almost always
independent from any other specification language choices made when
engineering a system, because of its specialized nature: specifying
and verifying the interactions of a multiparty protocol is
significantly different from specifying and verifying the behavior of
individual features or software modules. However, once a protocol is
verified, parts of its formal description can be embedded in the
source code specifications of appropriate system modules. For example,
the module that implements the receiving side of a vote casting
protocol can (and should) contain an associated formal model of the
appropriate protocol state machine and its state transformation rules;
this model can then be verified and validated in a modular fashion
alongside the rest of the source code specifications to provide
evidence that the system actually implements the verified protocol.

\tododmz{Need a section wrap-up and segue to the next.}

\section{Implementation Methodology}

\todoacf{Introduce context with NIST, ISO standards for methodology?}

\todoacf{Lay out the characteristics we want more explicitly, eg,
  freedom from flaws, repetability, correctness}

\todokiniry{Emphasize that this section covers best-practices of the
  industry, not just for safety-critical systems.}

\todokiniry{Give a big-picture of all of the facets we will cover in
  this section and how they relate to current certification
  standards.}

\subsection{Version Control}

Version control systems (VCS) manage changes between the versions of a
project as it evolves during the course of development. Revision
control is the preferred way to share software artifacts across a
team, but all software projects, even those developed by teams as
small as one person should use VCS.

In general, a developer uses the VCS first to ``check out'' the files
comprising a project into her ``working copy''. Then, after making
changes to those files, the developer ``checks in'' or ``commits'' the
changes to the VCS. After committing, those changes are available for
other developers to integrate into their working copies.

When different sets of changes have been made by multiple developers,
the VCS can merge those changes either automatically or by using
developer input to ensure the project remains consistent. The ability
to merge changes is critical for teams of developers who work
concurrently on a single project, and is the reason that file sharing
tools like Dropbox or Google Drive are an inadequate substitute for a
VCS.

VCS is particularly important for projects that must be audited. An
entry to a project log is created every time a developer commits their
work. Any file in the project can be inspected to show its provenance,
even down to the level of which line was committed by which developer
on what date. Some VCS tools also optionally allow for commits to be
cryptographically signed, offering assurance that, for example, the
changes have been audited by a trusted authority before being
integrated into the project~\cite{chacon2014pro}.

Moving from simple file storage to VCS is a tremendous improvement for
development, but poor use of VCS can negate many of the potential
benefits. For example, the log built from the commits of developers is
of less use to auditors if the changes in each commit are not clearly
associated with a particular new feature or bug fix. Likewise if
developers commit changes in a broken state, other developers'
productivity suffers and it becomes more difficult later to isolate
which commit introduced a bug. Each VCS supports multiple workflow
practices that should be adopted in order to limit these problems and
get the most benefit from VCS
use~\cite{atlassianworkflow,pilato2008version}.

\subsection{Issue Tracking}

During much of the software development and maintenance process, teams
add features and fix bugs. In an issue tracking system, each new
feature, each reported bug, and other discrete development tasks are
tracked as issues from creation to implementation, review, testing,
and integration. Issues can be organized by metadata such as assignee,
project milestone, priority, and task type. Issue trackers are
essentially to-do lists with additional structure that is specialized
to support effective software engineering.

These issues and their metadata give team members a view into the
status and health of the project, and facilitates the implementation
of project management methodologies \todoacf{reference management
  section}. For example, the issue tracker may automatically require a
code review step before an issue can be resolved. Issue trackers help
teams make fewer mistakes when following best-practice software
engineering workflows.

Team members can annotate issues with comments or attach supplementary
documents, creating a record of design decisions and thought
processes. This information is invaluable when investigating future
bugs or making subsequent changes to a design, and is often lost when
such discussions take place out-of-band and lose their association
with the task that motivated them.

Most issue trackers integrate with VCS in order to associate issues
with the source code changes that were made in order to complete those
tasks. When combined with the design discussions captured in issue
comments and attachments, this further enhances the ability of the
team to reliably understand and maintain the project in the future.

Issue trackers not only benefit the project team, but also often serve
as a first line of contact for users of a system when they encounter
problems. In projects with short timelines like election systems, it
is critical to incorporate feedback into development as quickly as
possible. Giving users or front-line support staff the power to create
issues directly makes the feedback loop very small.

Keeping a public issue tracker reduces duplicated effort across both
users and developers. If a system has a flaw, that flaw will likely
become apparent to multiple users, and duplicate reports are less
likely if users can check the issue tracker for other reports of
similar problems. The development team can then coordinate an
effective response to problems. The team can triage issues by
importance and urgency, discuss potential solutions, assign developers
to implement those solutions, and finally make sure the problem is
resolved and notify the users who originally reported the problem.

\subsection{Testing}

Software testing practices are a key component of any software
engineering methodology. Even when parts of a system are formally
verified, testing helps provide additional assurance that the system:

\begin{itemize}
\item satisfies its requirements,
\item behaves as expected for particular inputs,
\item works correctly in diverse environments,
\item and has sufficient performance.
\end{itemize}

Testing serves the key functions of uncovering flaws quickly, and
ensuring that previously-fixed flaws do not recur in later
versions. The assurance testing provides comes from performing these
functions. It is impossible for testing to reveal all possible flaws
in any realistic program---the number of possible inputs for any such
system is so large that an exhaustive test would effectively run
forever---but tests that successfully reveal and prevent recurrences
of some flaws provide more assurance than an untested system.

Formal verification and testing should not be viewed as opposing
alternatives, but rather as complementary techniques that together
provide assurance in the developed software. It is not feasible to
exhaustively test every possible input and every possible path through
any non-trivial program, while formal techniques offer guarantees over
all possible inputs. However, formal techniques usually cannot scale
to provide those guarantees for entire systems, and sometimes must
make simplifying assumptions about the environment in which software
runs, or reason about a simplified model of the actual system. Since
testing can use the real system in a real environment, it can uncover
flaws that are beyond the scope or capability of the formal
techniques.

Different testing practices provide complementary types of assurance;
no single testing practice is sufficient to cover the above
list. Instead, multiple types of testing should be used in
combination on every project.

\subsubsection{Unit Testing}

Unit testing exercises the smallest components (the ``units'') of a
program that are feasible to test. The granularity of unit tests
varies by programming language, but typically unit tests are small
enough to test the implementation within a single module, class, or
other per-file abstraction.

Unit tests are usually written to reflect the specification of the
unit under test. For example, a unit that implements a specification
of addition might have unit tests that check the associativity and
commutativity of the implementation. Most software specifications are
too abstract to translate directly into unit tests, so for a property
like associativity, developers must choose particular concrete values
to test. The shortcoming here is that developers might not choose the
particular values that would expose a flaw, and so the test suite
will succeed despite the presence of that flaw.

Developer intuition and understanding of the implementation increases
the likelihood that unit tests will exercise code containing a flaw,
but tests still cannot be exhaustive. Code coverage, the percentage of
lines of code exercised by a given test suite, is often used to
measure the effectiveness of unit tests. In practice, high coverage
percentages have not been shown to necessarily uncover more
flaws~\cite{inozemtseva2014coverage}, however more sophisticated
coverage measures such as branch coverage are more
promising~\cite{gligoric2013comparing}.

\subsubsection{Randomized Testing}

Manually-written tests can only exercise a small fraction of the
potential inputs to a program. When test cases can be generated
randomly, it is much cheaper to produce a large number of test cases
that can explore a larger fraction of potential inputs.

With randomized testing, developers specify a means for generating the
inputs to a test, and provide a function or ``oracle'' for evaluating
whether the test succeeded with those inputs. These test generators
can more closely mirror specifications than tests that use concrete
values, as they can instead make assertions about all possible
values.

For example, a unit test for an addition implementation might by hand
assert that $0+0=0$, $1+0=1$, and $2^{32}+0=2^{32}$, but a random test
could assert that for all integers $x$, $x+0=x$. Unless the range of
the input is very small, a random test will not provide an exhaustive
proof of the property it expresses. It can, however, easily provide
orders of magnitude more test cases than hand-written tests, and will
usually produce test cases that a developer might not think to add
intuitively. When a higher level of assurance is required, the
specification of a random test often translates directly to a logical
formula usable by formal techniques, easing the transition to a
verified system~\cite{swierstra2012xmonad}.

In the addition example above, it is straightforward to generate
random integers and check whether the results are correct. However,
randomized testing is situational, since an oracle is not always
straightforward to develop, and even the random input generator can be
quite complicated for complex input types. For example, random testing
has successfully been used to find flaws in C compilers. The
development of the C test program generator has itself been the
subject of extensive research~\cite{yang2011finding}, and the oracles
used are primarily other C compilers.

Because it is difficult to apply random testing to programs with
complex input types and no readily-available oracles, random testing
is most commonly found in unit tests. Random testing can still be
useful in such cases by relaxing the requirements of the input
generator and oracle, and simply observing whether a program crashes
or violates internal assertions when presented with a random, possibly
malformed input. This variant of random testing is known as
``fuzzing'', and should be used as a complement to other forms of
testing.

\subsubsection{Model-Based Testing}

Another method for automatically generating test cases is model-based
testing, where formal models of the program's behavior can be used as
test oracles, as guides for choosing test data, or both.

Formal models of program behavior can be used as test oracles by
compiling them into the software as \emph{runtime assertion
  checks}. For example, assume the specification for some function $f$
guarantees that some condition $x$ is true when it returns. If $x$ is
ever false when $f$ returns, $f$ does not satisfy its
specification. Any good set of unit tests for $f$ should detect this
divergence, and it would certainly be straightforward to write (by
hand) a test oracle that checked the value of $x$ after executing
$f$. However, in many cases $f$'s specification can be
\emph{automatically} transformed into a set of runtime assertions,
such that (in this case) an error is always raised if $x$ is false
when $f$ returns.  Compilers that perform such transformations are
available for several programming languages and corresponding
specification languages; some widely-used examples are Java and the
Java Modeling Language (JML), C\# and Code Contracts, C and the
Executable ANSI/ISO C Specification Language (E-ACSL), and Eiffel
(with its integrated specification language).

The set of runtime assertion checks derived from a formal
specification of a module effectively becomes a test oracle for that
module; in general, more precise specifications lead to better test
oracles. For a function $f$ with such an oracle, each possible input
to $f$ defines a test, and each test is run by simply calling $f$ with
that input. A test passes if all the runtime assertion checks pass and
fails if any runtime assertion check fails. If the specification of
$f$ restricts the set of possible inputs, tests that supply invalid
input are effectively meaningless and their output is ignored.

Of course, it is impractical to call $f$ with every possible input;
however, strategies such as randomized input data generation
(discussed above) and ``interesting'' input data generation (for
example, ensuring that all boundary conditions are tested for data
types with boundaries, such as numeric types) can be used to cover the
functionality of $f$ with a reasonable number of tests. Multiple test
frameworks use these techniques to automatically generate and run
model-based tests.

Formal models of program behavior can also be used as guides for
choosing test data; for example, some tools use constraint solving
techniques on programs and their specifications to ensure that test
data satisfies input constraints on the functions under test.
Techniques such as symbolic execution---analysis of the program to
determine what inputs cause what execution paths to be taken---can
also be used to choose test data, with the goal of achieving maximal
coverage using a minimal set of test cases. 

\subsubsection{Regression Testing}

In addition to unit and integration tests written to reflect the
specifications of a system, new tests should be added whenever a flaw
is uncovered and fixed. Flaws tend to recur in software for a variety
of reasons. The existence of the initial flaw implies that there is
some subtlety to that particular code, raising the baseline likelihood
of flaws. The fix applied to the code may have only fixed the flaw
under the limited set of conditions that were observed at the time,
for example in a bug report. The fix may also have depended on
assumptions about code elsewhere in the project, and the flaw can
recur once those assumptions change.

Running a regression test for every flaw in the project's history
assures us that those flaws are not present in the current software
artifacts, but for a long project, the weight of that history can make
the regression suite unfeasibly large. Many longer-term projects
therefore split their regression tests into multiple suites: a small,
quick suite to run before each code commit, a larger suite to run
every night, and sometimes a full suite that runs over weekends or
before major project milestones. Since a goal of testing is to uncover
flaws as quickly as possible, running tests less frequently is a
tradeoff, and prioritizing and minimizing the cost of testing is an
area of active research~\cite{yoo2012regression}.

\subsubsection{Integration Testing}

While unit testing and regression testing detects flaws on a
per-module granularity, integration testing detects flaws in the
system as a whole. This type of testing can expose flaws in the way
multiple modules interact, measure performance of the integrated
system, reveal environmental (e.g., configuration, operating system,
network) dependencies, and simulate the end-to-end experience of the
system's users.

The most basic integration test is simply to check that the complete
system can be successfully built. Once built, integration tests
exercise substantial functionality across multiple modules, often
simulating the actions performed by a user during an interaction with
the system and checking for expected outcomes. In this sense,
integration tests are frequently the first line of validation applied
to a system.

For example, one integration test might load a ballot, make
selections, change selections, and then cast the ballot. Another
integration test might follow the same steps, but then spoil the
ballot and repeat with a new ballot. While each of these individual
steps might concern only a single module, the combination of steps
helps expose potential problems with modules interacting, for example
if a ballot successfully loads but fails to allow the ballot to be
marked correctly.

In addition to simulating end-to-end functionality, integration tests
also test the suitability of the software in its intended
environments. This is critical for systems that are intended to work
with multiple operating systems, with or without a network, or with
specialized hardware peripherals like a ballot marking device. Making
the environmental assumptions explicit in integration tests also helps
prevent flaws from arising due to unstated assumptions. For example, a
developer using the latest version of Linux may inadvertently write
software that depends on that version of Linux and fails when run on
the version of Linux used in the deployed environment. It is
counterproductive and often infeasible for developers to use the
deployed environment (it may not even be a general-purpose computer),
and so the integration tests must accurately recreate that
environment.

\subsection{Continuous Integration}

The expense of fixing software flaws increases over time as other
parts of a program accumulate over time around those flaws. When a
flawed feature is new, developers have not had a chance to write other
code that depends on the flawed code. Once those dependencies exist, a
fix for a single flaw can have consequences that ripple outward across
the entire project, making the fix much more expensive. The cheapest
way to fix a flaw, then, is to discover the flaw as quickly as
possible.

Continuous integration (CI) and testing facilitate this by discovering
flaws as part of a regular, automatic process that is not dependent on
due diligence of individual developers. CI interleaves the quality
control process into the development process, rather than leaving it
as a separate phase for the end of a project after development has
finished.

CI tools automatically build and test the latest version of the
software in the VCS system on a regular basis, such as every night, or
after every VCS commit. Because the software is built from the VCS, it
is important for developers to frequently commit their work to the
VCS. The VCS integration ensures the tests are always run on the
canonical version of the software, and that any discovered flaws can
be linked to a particular version in the VCS system. Instead of
potentially having to search the entire codebase for the cause of a
flaw, isolating the failing version focuses the efforts of developers
on the set of changes introduced in that version, saving time.

CI systems substantially replace manual effort and the risk of manual
mistakes when releasing software. Since the CI system is building the
project on a nightly basis, it can also post the artifacts of those
builds for users and testers to quickly adopt. When an official
release like ``Version 1.0'' is ready for release, developers can
simply run the CI system to produce the final artifact. Because the
same system is responsible for both the continuous testing and
validation of the system and the creation of the final release, it is
less likely that the final release will have flaws that would have
been caught through earlier testing.

\subsection{Code Review}

Code review practices involve examining the results of the software
development process to find flaws, identify potential improvements,
and increase understanding of the software throughout the engineering
team. Reviews are also an opportunity to ensure that organizational
code style standards are met, and that the code and its documentation
is clear enough that it can be effectively communicated to others
during the review. This process, like discovering flaws during
testing and investigating issue reports, feeds back into an interative
development process to improve the quality of the final product.

Code review can be a manual process at varying levels of rigor. On the
formal end, processes like Fagan inspection require a line-by-line
inspection by many developers in an extended meeting, and catch a high
percentage of flaws~\cite{fagan2002design}. On lightweight end, code
review inherently occurs during pair programming, and can take place
informally via a developer-led walkthrough or simply an email to
colleagues requesting feedback. Formal inspections are more costly
than informal reviews, but may be more suitable for projects that
require concrete audit trails for accountability. Lightweight methods,
particularly pair programming, can find similar proportions of flaws
for lower cost~\cite{tomayko2002comparison} and has other knock-on
effects such as higher developer job satisfaction and improved team
dynamics~\cite{cockburn2000costs}.

Automated tools complement any form of manual code review. Lightweight
static analysis tools and code ``lint'' tools can help developers
avoid common coding mistakes and adhere to organizational style
standards. Very lightweight static tools can be run by individual
developers before committing their work to the VCS, and longer-running
analyses can be part of the continuous integration and testing
process. Such analyses are not substitutes for formal verification,
however, as they typically are meant to discover small-scale defects
and help developers avoid common pitfalls rather than proving overall
properties about the correctness of a system.

\subsection{Release Management and Lifecycle}

Release management is the process through which software moves from
implementation through testing, validation, and verification into a
finished product that can be used in its intended environment. Release
management is primarily focused on the smooth integration of the
different aspects of the project, and on adhering to practices that
make releases repeatable, reliable, and auditable.

Release management and VCS workflows are tightly connected. For
example, release managent would be responsible for creating a new
release branch in the Git Flow model~\cite{atlassianworkflow},
imposing a feature freeze (no new features, only bug fixes) on that
branch, and eventually tagging that branch upon release and merging it
back into the main development branch.

For a project that delivers software as a service on a web server,
release management would be responsible for deploying the software to
production servers. For software delivered as a binary download or CD,
release management would be responsible for cryptographically signing
and distributing the binary. In both of these cases, the release
manager serves as the final line of quality assurance before the
software is used in its intended environment, and so she must be
fluent enough with all aspects of the project and its processes to
release software only once the processes have been faithfully
executed.

\subsection{Testable Documentation}

Documentation of the design, implementation, and use of a software
system is a standard requirement in software engineering
methodologies. However when a system is under development and rapidly
changing, documentation can lag behind and fall out of step with the
latest version of the software, leading to errors and confusion.

Where possible, documentation should be machine-testable (or even
machine-generated) and integrated with the VCS rather than being a set
of static resources maintained independently of the software
itself. Much as testing gives the advantage of early software flaw
detection, testable and generated documentation is less likely to
become inconsistent with the software it describes.

The form of testable documentation varies depending on the granularity
of the documentation and the underlying technologies used by the
project. For example, Business Object Notation
(BON)~\cite{walden1995seamless} can be used as analyzable
documentation at the specification, design, and architecture level.

At the level of code modules and interfaces, documentation should be
concretely executable like the ``doctest'' features available for
specification languages like JML (using its \texttt{examples} pragma)
and programming languages like Python and
Haskell~\cite{python3doctest}. Documentation in this style contains
short examples that illustrate the expected use of a system and its
expected response, for example in Python:

\begin{lstlisting}[language=Python]
"""
This is the fibonacci module. It provides the function fib which
returns the nth fibonacci number, where n >= 0.

>>> fib(0)
0
>>> fib(10)
55
>>> fib(-1)
Traceback (most recent call last):
    ...
ValueError: n must be >= 0
"""
\end{lstlisting}

These executable tests should supplement, not replace, traditional
prose documentation. Since they amount to a form of unit test, they
suffer from the same limitations. They usually only exercise a handful
of concrete values, and cannot test non-functional properties like
expected performance or thread safety.

\subsection{Reproducibility and Automation}

A team can implement many of the techniques in this section
manually. Tests can be run by hand on developers' machines, code can
be sent out for review by email, issues can be tracked in mailing list
posts, and a release manager can build and package release artifacts
by hand for each supported platform. Each time a step in a process
must be manually performed, the probability of human error increases,
and reproducing steps for later quality assurance and troubleshooting
becomes harder.

A manual operator might skip a step or perform a step out of order,
for example running the test suite before integrating the latest
changes from the VCS. The operator might also introduce new steps that
seem necessary and obvious, but unless recorded will make it very
difficult to reproduce or audit the process in the future. Finally,
manual execution of a process, even if done correctly, takes much
longer than automated execution.

To prevent errors, improve reproducibility, and make development more
efficient, processes should be automated as much as possible. The
methodologies described in this section all support automation and
reproducibility or can themselves be automated to a degree, but some
play key roles:

\paragraph{Version Control}

The version control system is a linchpin of automation on a
project. The versions it manages are the starting point for automated
and reproducible continuous integration, testing, and software
releases. The VCS can itself trigger automated processes, for example
it could trigger a run of the test suite after every commit.

Any automated process should be run in the context of a particular VCS
version, and any artifacts produced by these processes should refer to
this version. For example, if software has a built-in bug reporting
feature, those reports should automatically include the version at the
time the software was built. This allows engineers to easily reproduce
the exact circumstances where the user discovered a flaw.

Version control can only improve reproducibility when all of the
relevant inputs to a process are managed in the VCS. For example, a
software build process that depends on a configuration file in the
user's home directory would not be reproducible on a different
computer without that home directory.

\paragraph{Testing}

Manual testing can play an important role when evaluating a system,
but any realistic system requires more tests than are feasible to
perform manually. Even if the contents of a test suite are automatic,
if that test suite is only run manually it will often be skipped,
particularly when it takes a long time to run. Automating both the
tests themselves and the running of those tests ensures that they will
be run on a consistent basis, and that the results will be
reproducible and traceable to a particular version.

\paragraph{Continuous Integration}

Continuous integration is another linchpin of project
automation. Since CI tools are designed to automatically run on a
regular basis and offer integration with the VCS, other processes are
usually automated by using these tools. For example, after building
the integrated software, a CI tool should run the test suite and
archive the built artifacts for subsequent release management.

\paragraph{Release Management}

No process has as many moving parts or cross-project concerns as
release management, making manual release management extremely
error-prone. The entire process from checking out a version from the
VCS to deploying the final release artifacts should be as automatic as
possible. The manual intervention should amount to simply deciding
which version to release, and checking before the final release that
the automated process performed as expected.

Because automation is inexpensive when using continuous integration,
it is a good practice to have CI tools perform parts of the release
management process on a regular basis, even when software is not ready
for a release. If the process of producing a release is the same as
performing an ordinary nightly build and test, it is less likely that
problems will arise only at the release stage when it is much more
costly to address those problems.

\section{Technologies}
\label{sec:technologies}

Here, we provide some recommendations about specific technologies
that, at present, are well suited (and in some cases, \emph{not} well
suited) to performing rigorous software engineering of the type we
have described. These recommendations are based on experience applying
these methods over the last 15+ years; however, they are not meant to
be a rigid set of rules or to unconditionally exclude technologies not
mentioned here. The landscape of software development languages and
tools is constantly changing; new languages and tools appear, while
old languages and tools disappear, are marginalized, or evolve in
possibly surprising ways.

\subsection{Domain Modeling}

\todokiniry{Some mention of RAISE, VDM, Z, and B are necessary.}

For domain modeling, we recommend the Business Object Notation
(BON)~\cite{walden1995seamless} and Extended Business Object Notation
(EBON)~\todokiniry{should we have a reference for EBON? if so, which
  one?}. BON is both a language and a design/refinement method
encompassing informal domain analysis and modeling, formal modeling,
and implementation-independent high- and medium-level
specification. BON has a well-defined semantics, is easy to learn and
write (especially the informal models, which are effectively
collections of simple English sentences), and has equally-expressive
textual and graphical notations. BON was originally developed for use
with the Eiffel programming language and is well-supported by the
EiffelStudio tool suite; however, it can be used with other
specification and implementation languages. EBON adds additional
\emph{semantic properties} to BON, allowing BON to express properties
relating to domains such as concurrency, ownership, responsibility,
bug tracking, literate programming, and version control.

We recommend BON over the Unified Modeling Language (UML), which has
seen much wider use in the software industry, for several
reasons. First, BON's equivalently expressive textual and graphical
notations are easy to work with and manipulate. UML supports only a
graphical notation, though there is also an unsupported official
``Human-Usable Textual Notation''~\cite{HUTN}; it was last updated in
2004, reflects only the version of UML that was current at the time,
and is not as expressive as the graphical notation. Currently, there
are at least a dozen different and mutually-incompatible textual UML
dialects supported by various tools, none of which are as expressive
as the UML graphical notation and most of which have significant
readability issues.

Second, BON's semantics are an integral part of the language and
method and are easily understandable. By comparison, UML effectively
has no semantics; the Object Constraint Language (OCL)---the
specification language typically associated with UML models---is a
very complex expression language, and is not an integral part of UML.

Third, BON was explicitly designed to support \emph{seamlessness} and
\emph{reversibility}. Seamlessness is the property that allows a BON
model to be smoothly (and, in many cases, completely automatically)
refined to lower-level specification languages, and further to
executable implementations. Reversibility is the property that allows
consistency to be maintained between the BON model, which is an
important part of the system documentation, and the resulting
implementation---when the implementation is changed, that change can
be (again, often completely automatically) propagated back up to the
BON model. Seamlessness and reversibility are both useful properties
for ensuring that the final software product accurately reflects the
original domain analysis and architecture design.

Fourth, BON supports high-level domain modeling using natural
language, making it easy to communicate models not only among software
developers but also with other stakeholders in the development
process. The BON representation of the E2E-VIV requirements in
\autoref{appendix:bon_requirements} consists almost entirely of simple
English sentences; it is therefore far more accessible to a wide
audience than an equivalent set of UML diagrams would be.

Finally, BON is \emph{simple}. Its specification is a fraction of the
size of the UML specification, and its graphical representation is
significantly less complex. For example, arrows in BON diagrams only
have two possible appearances (a single or double line, with a single
filled arrowhead) as compared to UML's proliferation of arrow types
(solid and dashed lines, filled and empty arrowheads, diamonds, and
circles, and additional connection markings). 

\subsection{Specification Languages}

In addition to its use for domain and architecture modeling, BON can
also be used as an architecture specification and concept
specification language and we recommend its use as such. Once written,
BON specifications can be refined further into architectural
specification languages for expressing detailed architectural
properties, source code specification languages for integration with
particular implementation languages, and protocol specification
languages for formalizing interactions within the system.

UML is the most commonly used tool for architecture specification, but
is not well suited to rigorous software engineering because it lacks
semantics. We recommend using BON for high-level architecture
specification and a formal architecture specification language with
strong semantics, such as the SAE Architecture Analysis and Design
Language (AADL)~\cite{AADL}, for low-level architecture
specification. \todokiniry{Reasonable?  Other architecture
  specification languages?}

\tododmz{Mention of SysML is appropriate here.}

Programming languages for which there is an obvious best choice of
source code specification language include Java
(JML~\cite{JMLReferenceManual}), C\# (Code
Contracts~\cite{CodeContracts}), and C (ACSL~\cite{ACSL}). Some
implementation languages, such as SPARK~\cite{SPARK2014} (a dialect of
Ada) and Eiffel, have integrated specification languages. These
specification languages all have several features that we consider
essential for efficient and effective use: straightforward syntax and
semantics, integration with widely-used software development
environments, and tool support for performing analysis and
verification. In the case of JML and Eiffel, tool support is also
available for automated reversible refinement from BON.

In any given project, different parts of the architecture may be
implemented in different languages. For example, it might be
appropriate to implement some computation-intensive parts of a design
in a language like C while implementing the rest of the design in a
language like Java or C\#. Thus, multiple source code specification
languages may be used in a single project, though we recommend that
high level specifications be written in a single language to the
extent possible.

There are also several language-neutral specification languages and
associated development tools, such as Alloy~\cite{Alloy},
VDM~\cite{VDM}, Z~\cite{Zed} and Event-B~\cite{Abrial10}, which
themselves support refinement to various implementation
languages. These can be used effectively, either by themselves or
alongside other specification languages, in a rigorous software
engineering process. 

\tododmz{Must add mention of EasyCrypt.}

Finally, any of several protocol specification languages can be used
to specify the interaction protocols within the system. These include
the High-Level Protocol Specification Language (HLSPL) used by
Avispa~\cite{Avispa}, typed $\pi$-calculus as used by
ProVerif~\cite{ProVerif} and CryptoVerif~\cite{CryptoVerif},
Casper~\cite{Casper}, and Scyther Protocol Description Language
(SPDL)~\cite{Scyther}. Each language is directly tied to a protocol
verification tool, and each tool has its own strengths and weaknesses
with respect to verifying different types of protocol; thus, the
choice of tool for verifying a given protocol dictates the choice of
language for specifying that protocol (or vice-versa), and it may be
appropriate to use different tools for different protocols within the
system.

\subsection{Implementation Languages}

The choice of implementation language is clearly important, and there
are many possible choices; hundreds (possibly thousands) of
programming languages exist and dozens of those are actually viable,
with widespread adoption, tool support, and support communities. In
most cases, language choice determines programming style: for example,
Eiffel imposes an pure object-oriented style while Haskell imposes a
pure functional style. Language choice also determines the set of
existing functionality, in the form of standard libraries accompanying
the language or well-established external libraries available for use
with the language, on which developers can rely while building the
implementation. 

Implementation languages are distinguished by different programming
styles, different methods for error handling, different security
guarantees, and different ways in which programmers can make mistakes
(both minor and catastrophic). For rigorous software engineering, we
recommend languages with strong type systems that actively prevent
programmers from making any of a large class of errors. We also
recommend languages that automatically handle memory allocation and
deallocation, which prevents another large class of errors and many
potential security issues. Finally, we recommend languages that either
have good specification and verification tool support or are designed
explicitly for the implementation of high-assurance software. It is
certainly possible to implement reliable software in languages that do
not have all these features---for example, code can be written in a
safe subset of C, specified with ACSL, and verified with various
tools---but it is significantly more difficult to do so.

The nine implementation languages we currently recommend for rigorous
software engineering are (in alphabetical order) C (in a safe subset
such as $C_{0}$~\cite{C0} or that which VST~\cite{VST} can reason
about), C\# (excluding unsafe code), Eiffel, Erlang,
Gallina~\cite{Gallina}, Haskell~\cite{Haskell}, Java,
OCaml~\cite{OCaml}, and SPARK. \todo{Is this list complete or did I
  forget one? Does Erlang belong? I think it does, but it doesn't
  quite fit the mold of the other languages in terms of
  verifiability. -dmz} No single implementation language is ideal for
every project, and it is often appropriate to use multiple languages
in the same project; for example, it would be reasonable to write the
computational core of a voting system in a pure functional language
like Haskell and the voter-facing user interface components in an
object-oriented language like Java.

\subsection{Static Analysis}

Static analysis tools process the system's source code to provide
information about the system without executing the code.  There are
many forms of static analysis, ranging from simple syntactic checks to
full functional verification of source code specifications. The
following set of recommendations is not exhaustive, and in particular
does not discuss specific tools covering all the recommended types of
static analysis for all the recommended programming languages; many
useful static analysis tools that can be effectively deployed in a
rigorous software engineering process are not mentioned here.

At least one static analysis tool should be used to enforce some set
of code style and formatting guidelines, so that the implementation's
code base is consistently readable. Examples of tools that enforce
such guidelines are Checkstyle~\cite{Checkstyle} for Java and
StyleCop~\cite{StyleCop} for C\#. The style enforcement tool(s) should
be run automatically and frequently, either within each developer's
environment (for example, a Checkstyle plugin can run Checkstyle every
time a file is saved in any of the commonly-used Java IDEs) or as part
of the version control or continuous integration processes.

Static analysis tools should also be used to detect ``code smells''
and other problematic aspects of the implementation. The presence of
significant amounts of duplicate code in multiple locations in the
implementation and the excessive use of numeric literals that should
be declared as symbolic constants are examples of code smells; other
problematic aspects might (depending on the implementation language)
include memory leaks, buffer overflows, and concurrency problems.
Static analysis tools that can detect these issues include
FindBugs~\cite{FindBugs} and PMD~\cite{PMD} for Java,
ReSharper~\cite{ReSharper} for C\#, Eiffel
Inspector~\cite{EiffelInspector}, and SPARK Pro~\cite{SPARKPro}.

Finally, static analysis tools should be used to verify protocol
specifications and source code specifications. As previously
mentioned, the choice of tool for verifying protocol specifications is
dictated by the choice of protocol specification language. Source code
specifications can be verified with extended static checking tools,
such as OpenJML~\cite{OpenJML} for Java with JML specifications,
Frama-C~\cite{Frama-C} for C with ACSL specifications, and SPARK
Pro. In addition, source code can be proven equivalent to a reference
implementation or to a formal model using tools like the Software
Analysis Workbench (SAW)~\cite{SAW}; this type of analysis is
particularly useful when verifying the correctness of cryptographic
algorithms, which are pervasive in E2E-VIV systems.

\subsection{Dynamic Analysis}

Dynamic analysis tools monitor a running system to measure aspects of
its operation and detect undesirable behavior. There are many
different forms of dynamic analysis and many dynamic analysis tools;
like the static analysis recommendations above, the following set of
recommendations is not meant to be exhaustive.

One important form of dynamic analysis that we strongly recommend
using when possible is runtime assertion checking (RAC), previously
mentioned in associated with model-based testing. RAC compiles runtime
checks for specification conformance into the implementation, which
ensures that any runtime violations of the source code specifications
will be detected and reported. OpenJML for Java, Code Contracts for
C\#, SPARK 2014, and Eiffel all support RAC compilation with their
associated specification languages.

Another form of dynamic analysis that can be useful is
\emph{specification inference}. Tools like Daikon~\cite{Ernst2007},
which works on Java and C\# programs, can infer likely specifications
(particularly invariants) that may have been missed during the
specification refinement process. If these inferred specifications are
valid, and are added to the source code specifications, they can
assist ESC tools in verification and provide additional runtime
checks; in some cases, the additional specifications can allow ESC
tools to verify parts of the implementation that they otherwise could
not.

\emph{Coverage analysis} is another useful form of dynamic analysis,
particularly when used in conjunction with automated testing. Coverage
analysis tools, such as EMMA~\cite{EMMA} for Java,
OpenCover~\cite{OpenCover} for C\#, and
GNATcoverage~\cite{GNATcoverage} for SPARK, can provide information
about ``how much'' of the code was executed during a particular
run. ``How much'' can be expressed using several different metrics,
including \emph{statement coverage} (the percentage of the source
statements actually executed), \emph{branch coverage} (the percentage
of the program branches that were taken during the execution), and
\emph{path coverage} (the percentage of possible execution paths that
were taken during the execution). When used in conjunction with
automated testing these coverage metrics can give a rough idea of test
suite quality; for example, a test suite that does not exercise the
entirety of the system by at least one metric is clearly not as good
as a test suite that exercises the entirety of the system by all
coverage metrics. 

Dynamic analysis can also be used to detect issues related to memory
(leaks, corruption), concurrency (deadlock, spinning, data races),
resource allocation (unclosed sockets and files), and security (buffer
overflows and other vulnerabilities). Some of these issues are already
mitigated by the languages and other forms of analysis we
recommend---for example, none of the languages we recommend allow
programs to be vulnerable to buffer overflow attacks in the same way
that traditional C and C++ programs can be---but it is useful to run
dynamic analysis tools to detect the ones that are not. There are too
many such tools, of too many types, to list here.

Finally, dynamic analysis can be used to \emph{profile} the executable
code, monitoring it to determine which parts are executed most
frequently and to find performance bottlenecks. This allows developers
to gather empirical evidence for use in comparing multiple
implementation choices (e.g., what data structure variant to use for a
particular part of the system's data model), rather than blindly
guessing at the consequences of implementation decisions. It can also
help to direct optimization efforts when tuning the system for
performance late in the development process. All the languages we
recommend have associated profiling tools, many of which are
integrated into their respective development environments.

\subsection{Model Checking}

\tododmz{Alloy, PVS, SPIN, UPPAAL}

\subsection{Constraint Solving}

\todo{It's not clear to me that we really need a section on constraint
  solving; it seems to be covered by static and dynamic analysis and
  testing.}
\todo{Agreed. Kill it.}

\subsection{Model-Based Synthesis}

\todokiniry{I'm not really up-to-date on the tools for model-based
  synthesis. -dmz} 
\todokiniry{The main options here are VDM, Event-B, and Coq.}

\subsection{Version Control}

For version control we recommend using either Git or Mercurial. Both
are current-generation distributed version control systems, supporting
various models of collaboration, and both are well-supported. Both
also have associated services---GitHub~\cite{GitHub} for Git,
BitBucket~\cite{BitBucket} for Mercurial, and
SourceForge~\cite{SourceForge} for either one---that provide
repository hosting, issue tracking, web hosting, and wiki
functionality. Git is currently the more popular of the two by a
significant margin, but they are both good options for new projects,
and the choice between them is mainly one of developer personal
preference.

We recommend against using older, centralized version control systems
such as Subversion and CVS. In general, their mechanisms for handling
concurrent development and maintenance of multiple versions of
software are significantly more awkward than those of the
current-generation systems, and the single synchronization point
inherent in their centralized architectures makes it more difficult
for developers to work offline.

\subsection{Issue Tracking}

There are several good choices for issue tracking. If the version
control repository is hosted by one of the well-known hosting services
(GitHub, BitBucket, SourceForge), the obvious choice is to use the
issue tracker integrated with that service. Otherwise, there are
several choices, both proprietary and open-source, for either cloud
hosted or locally installed issue tracking.

Atlassian's JIRA~\cite{JIRA}, JetBrains's YouTrack~\cite{YouTrack},
and Fog Creek Software's FogBugz~\cite{FogBugz} are all hosted issue
trackers that integrate with both Git and Mercurial repositories. JIRA
and YouTrack are also offered as standalone commercial products that
can be installed and maintained locally. All three have roughly
equivalent capabilities and the choice among them, like the choice
between Git and Mercurial, is largely a matter of developer
preference.

Trac~\cite{Trac} (along with its spinoff project Apache
BloodHound~\cite{ApacheBloodhound}) and Redmine~\cite{Redmine} are
open-source issue tracking systems supporting both Git and Mercurial,
which can be installed locally and used for free. They are also
reasonable choices when installed and managed appropriately, though
their user interfaces are generally less polished than those of the
commercial options.

\subsection{Testing}

Testing is an integral part of the development process, and many
different tools exist to automate the generation of unit tests and the
execution of unit, regression and integration tests. In general we
recommend that tests be run as often as possible, preferably both as
part of a continuous integration process and by individual developers
as they implement specific parts of the system. 

The use of a test automation framework is essential. Many unit test
automation frameworks take the same form, originated by the
SUnit~\cite{SUnit} framework for Smalltalk in the late 1990s, and are
typically referred to as \emph{xUnit} frameworks. Test cases are
written in a special format, combined into test suites, and executed
by a test execution program that gathers information about which tests
pass, which tests fail, and how any test failures occur. This
information is then presented to the developer in a textual or
graphical format.  JUnit~\cite{JUnit} and TestNG~\cite{TestNG} for
Java, NUnit~\cite{NUnit} for C\#, and HUnit-Plus~\cite{HUnit-Plus} for
Haskell are examples of such frameworks. We strongly recommend the use
of an xUnit framework for test automation, especially for complex
scenario tests that cannot be automatically generated by other testing
tools.

Randomized testing tools, such as the original
QuickCheck~\cite{QuickCheck} for Haskell and the many QuickCheck-like
tools developed for other languages (most of which are named
``QuickCheck for X'' or ``X-QuickCheck''), automatically generate
random unit tests. Some of these tools are guided in their test data
choices by performing constraint solving on existing source code
specifications; others require manual guidance on the part of the
developer. We recommend that random testing be used in most projects,
especially for automatic generation of simple unit tests that would
otherwise require significant developer effort.

Since all systems implemented using rigorous software engineering
techniques have at least some formal specifications, we recommend that
some form of model-based testing be used. Several test frameworks use
model-based testing techniques to automatically generate and run
tests, including JMLUnitNG~\cite{ZimmermanNagmoti10} for Java/JML,
AutoTest~\cite{AutoTest10} for Eiffel, and PEX~\cite{PEX08} for C\#.

\tododmz{Add a discussion of fuzzing, particularly security fuzzers
  like \url{http://lcamtuf.coredump.cx/afl/} and John's work at
  \url{http://www.cs.utah.edu/~regehr/papers/}}

\subsection{Roots of Trust}

\tododmz{Get a paragraph or two from Adam Wick or Dave Lamkins about
  this topic.}

\section{Evidence-based Elections Technology}

\tododmz{Reflect upon the fact that no voting systems in existence
  today use even the basics that we have covered. Reflect on why this
  is the case: lack of capability in existing vendors, no pressure
  from NIST/EAC to do better, etc. Reflect upon how different this is
  in safety-critical domains like at JML and Airbus.}

\subsection{Measuring and Assessing Quality}

\subsection{Interpreting Evidence for the Non-expert}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "report"
%%% End:
