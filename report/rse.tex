\chapter{Rigorous Software Engineering\ifdraft{ (Joe K./Dan/Adam) (95\%)}{}}
\label{cha:rigor-softw-engin}

Sound engineering practices are the foundation for building reliable
and secure software systems. This is particularly true for critical
systems that must be trusted to perform important tasks correctly, and
where the consequences for failure threaten life, property, or
political system integrity.

The engineering of any software system can be roughly divided into
four stages: specification, implementation, verification/validation,
and maintenance. These stages are not mutually exclusive; for example,
a specification can be refined during implementation and
verification/validation can occur continuously during implementation
and maintenance.

Specification is the most important stage, particularly when building
critical systems: it informs the entire development process by
determining what must be built and what it must, and must not,
do. Even if a system works perfectly, it is impossible to provide
convincing evidence of its correctness without using and consistently
maintaining a high-quality, accurate specification throughout the
development process. Such evidence is essential for critical systems
to which we entrust our lives, money, or votes.

Implementation is the transformation, either automatic or manual
(typically some of each), of a system's specification into executable
code. The quality of an implementation is heavily influenced by
choices of programming languages, coding conventions, and supporting
tools, some of which are driven by choices made during specification.

Verification and validation are closely related, but independent,
procedures. Verification is an evaluation of whether the
implementation is a correct realization of its specification, while
validation is an evaluation of whether the implementation is
satisfactory to external stakeholders. Verification and validation are
typically carried out continuously and automatically during the
development process, using a variety of tools and techniques; the
results of verification and validation are the primary sources of
evidence for the implementation's correctness.

Maintenance includes tracking and fixing defects discovered in the
implementation, modifying the specification and implementation as
required by technical issues or feedback from external stakeholders,
and adapting the implementation to new hardware and software platforms
and new deployment environments. Maintenance is closely linked with
verification and validation, since new evidence of the system's
correctness must be provided when changes are made to either its
specification or its implementation.

This chapter introduces rigorous software engineering techniques for
these stages of the software development processes. We particularly
focus on specification, verification, and validation, since
implementation and maintenance are widely discussed throughout the
software engineering literature; however, we do describe specific
implementation and maintenance techniques that are particularly
important when developing critical systems. Finally, we recommend
specific technologies to use when constructing critical systems and
briefly discuss the current state of elections technology in light of
our recommendations.

\todo{Fix that last sentence depending on what the end of the chapter
  actually ends up looking like, and figure out what to do about the
  word ``we'' throughout.}

\section{Informal Specifications}

An \emph{informal system specification} is a human-readable
description of the system's purpose, functionality, and high-level
design. Informal specifications should be understandable not only to
the system's developers, but also to other stakeholders: clients for
whom the system is being developed, users of the system,
administrative staff who maintain the system, and auditors who
evaluate the system.

A complete informal system specification consists of a domain model, a
set of requirements and scenarios, and a set of concept
specifications.

\subsection{Domain Models}

A \emph{domain model} of a system identifies the various concepts
(also called \emph{classes} or \emph{classifiers} in some domain
modeling techniques) in the system, their attributes and roles, and
the client and inheritance relationships among them. Domain models can
be expressed in various ways, both textual and graphical. Regardless
of their form, they are essentially lists of concepts with associated
brief human-readable descriptions of themselves and their
relationships to other concepts. A good domain model provides a shared
vocabulary and gives a big picture view of the concepts involved in
the system upon which all the stakeholders can agree.

For example, the domain model for E2E-VIV systems (see
\autoref{appendix:domain_model}) includes, among many others, the
concepts ``election'', ``contest'', ``ballot question'', and
``choice''. It describes an election as ``a formal process of
selecting choices in one or more contests'', and says that a ballot
question is a specific type of contest, namely ``a decision among two
or more courses of action''. The relationship between a ballot
question and a contest is an inheritance relationship: a ballot
question \emph{is a} specific type of contest, with a certain set of
characteristics. One relationship among elections, choices, and
contests is that an election \emph{has}, or \emph{is a client of}, at
least one choice and at least one contest, because the description of
an election explicitly refers to choices and contests.

Importantly, while a good domain model comprehensively identifies and
defines the concepts and relationships in a system, it does not
otherwise constrain the realization of those concepts and
relationships or the actual behavior of the system. The description of
``ballot question'' does not specify \emph{how} a decision among two
or more courses of action is made, and the description of ``election''
does not specify \emph{how} choices in contests are selected. Such
constraints on the realization and behavior of the system are
described in general terms in requirements, scenarios, and informal
concept specifications, and are precisely expressed in formal
specifications.

\subsection{Requirements and Scenarios}

A system's \emph{requirements} are, essentially, the statements that
must be true of the system's implementation when it is
complete. Requirements are typically phrased as simple
natural-language sentences, each of which expresses a single testable
property of the system. ``The e-voting system shall maintain the
privacy of individuals.'' and ``All voter authentication secrets must
be changed at least once in every election cycle.'' are examples of
E2E-VIV requirements; the full set of requirements is described in
\autoref{chapter:required_properties}, and the informal requirements
document itself is in \autoref{appendix:bon_requirements}.

There are several different kinds of requirement, but they can be
broadly divided into two basic types: \emph{functional} and
\emph{non-functional}. Functional requirements are those that specify
how the system must behave and what outputs it must generate given
certain sets of inputs under certain conditions. For example, one
functional requirement of E2E-VIV systems is that only eligible voters
may cast ballots. Non-functional requirements are those that specify
overall characteristics of the system, such as a lower bound on its
availability or an upper bound on its cost. For example, one
non-functional requirement of E2E-VIV systems is that they must be
Open Source.

\emph{Scenarios} are descriptions of interactions among the entities
in the system, or between the system and its environment. A scenario
is typically expressed as a short paragraph describing the actions of
the entities involved, though numbered lists (to indicate a sequence
of operations) or communication diagrams (to indicate the flow of data
during a scenario) are also reasonable representations. A scenario may
also be expressed as a collection of requirements that, together,
describe the behavior of a part of the system under certain
circumstances.

It is important that requirements and scenarios describe not only the
\emph{ideal} behavior of the system, but also how the system deals
with situations that are less than ideal. System failures,
communication disruptions, data corruption, and malicious attacks of
various kinds should all be addressed in a system's requirements and
scenarios. It is not necessary to specify these at a level of detail
that provides explicit sequences of steps to recover from any given
failure or attack; rather, they can be addressed in general terms. For
example, one requirement of E2E-VIV systems that addresses system
failures is ``If service goes down for any reason other than regional
natural disaster or malicious attack, service must be restored in no
more than 10 minutes''.

As the system is built, \emph{traceability} of the requirements and
scenarios is critical. It must be possible to identify, for every
requirement and scenario, the related parts of the formal
specification and implementation, and the links among informal
specification, formal specification, and implementation need to be
kept current when any of them change. Moreover, it must be possible to
identify one or more system tests demonstrating that the system
implementation adequately addresses each requirement or scenario. This
should preferably be done automatically as part of testing and
continuous integration processes, in a way that makes it clear to
developers which, if any, requirements and scenarios remain to be
addressed.

\subsection{Concept Specifications}

A \emph{concept specification} is an informal behavioral description
of a concept in the system's domain model. Concept specifications help
to clarify what the roles and responsibilities of the concepts in the
model are with respect to the overall functionality of the system. A
typical concept specification is a set of English sentences called
\emph{queries}, \emph{commands} and \emph{constraints}, describing
respectively what information the concept possesses, what the concept
can do, and what limitations exist on its behavior. 

A query is phrased as a simple question, such as ``How many votes have
been cast?'' or ``Is this voter eligible to vote in this contest?'',
that the concept must be able to answer. Queries provide an informal
description of the data encapsulated in the concept, because the
concept must contain all the data necessary to answer any of the
queries that may be posed to it.

A command is phrased as an exclamation, such as ``Cast this vote!'' or
``Log this user out!'', describing an action that the concept must
take. This provides an informal description of the behavior
encapsulated in the concept, because the concept must be able to take
all (and only) the actions described by its commands.

Finally, a constraint is phrased as a declaration, such as ``A voter
must be eligible for this election to cast a vote.'' or ``Only users
that are logged in may be logged out.'', describing the circumstances
under which queries and commands may be issued. This provides an
informal description of the conditions under which queries and
commands may be issued to a concept. 

An informal concept specification has two main purposes: first, it
clarifies the roles, responsibilities and relationships of a concept
in the system. The process of creating informal concept specifications
may lead to the realization that some necessary concepts are missing
from the domain analysis, that some single concepts are actually
multiple related concepts that need to be described separately, or
that multiple concepts that were thought to be distinct are actually
aspects of the same concept. Second, it serves as a ``template'' for
developers implementing the concept, and---with appropriate tool
support for the formal specification and implementation
languages---can be used to automatically generate initial formal
specifications and implementation frameworks.

\section{Formal Specifications}

A \emph{formal system specification} is a machine-readable,
mathematical description of the system's functionality and design. It
is a refinement of an informal system specification, and there must be
a demonstrable and traceable correspondence between the informal and
formal system specifications. Formal specifications are meant to be
used both by the system's developers and by the various tools employed
during the development and testing process to validate the system
against its specification. They may also be used to automatically
generate partial or full implementations of system components, create
and run test suites, and generate part of the system documentation.

A complete formal system specification consists of architecture,
concept, source code, and protocol specifications; some of these, such
as the architecture specification, may be minimal for simple
systems. As we describe these types of specification, we also
introduce the verification techniques that are applicable to each.

\subsection{Architecture Specifications}

An \emph{architecture specification} is a precise description of the
system's architecture, formalizing both the relationships among the
concepts in the system and the relationships between these concepts
and the physical implementation of the system. 

Architecture specifications describe both the software architecture
and the hardware/network architecture on which the software runs. A
software architecture specification describes how many instances of
each concept exist within the system, the communication patterns among
the concepts, and any containment relationships among the concepts. It
also describes, at least at an abstract level, parts of the overall
software architecture that are external to the system under
development. For example, it may describe the relationships between
the concepts in the system, the services provided by the operating
system on which the software will run, and any external services (such
as databases or web servers) on which the software depends.

A hardware/network architecture specification describes the
relationships among the various hardware components in the
system. This includes the services provided by each component and the
network connectivity and communication patterns among them, as
discussed in \autoref{section:primary_architectural_variants}.

Some research has been done with respect to verifying software
implementations or hardware simulations against architectural
specifications~\cite{Linhares07, Abdoul08} and some tool support
exists for such verification; however, it is currently infeasible to
verify a distributed hardware and software implementation against an
architectural specification.

\subsection{Concept Specifications}

A \emph{formal concept specification} is a formal behavioral
description of a concept in the system's domain model. Formal concept
specifications are refinements of informal concept specifications, as
described in the previous section. It is possible for multiple
informal concept specifications to refine to the same formal concept
specification; for example, two informal concept specifications
``list'' and ``queue'' might both refine to the formal concept
specification ``linear data structure''.

Like informal concept specifications, formal concept specifications
have queries and commands; in formal concept specifications, these are
expressed as typed interfaces to the concept called
\emph{features}. For example, a formal specification of the
``election'' concept might refine the informal query ``How many votes
have been cast?'' with a feature of type \texttt{integer} called
\texttt{number\_of\_votes\_cast}, and the informal command ``Cast this
vote!'' with a feature of type \texttt{vote -> void} (that is, taking
a vote as a parameter and returning no result) called
\texttt{cast\_vote}. It is possible for a single query or command to
refine to multiple features of different types; for example, a ``Log
in!''  command might be implemented as two features, one taking
username and password parameters and the other taking an
authentication token parameter.

Features may be restricted, accessible only to a given concept or
group of concepts, or unrestricted, accessible to the entire
system. It is possible for a concept to have restricted features that
are not direct refinements of its informal queries and commands. For
example, the ``election'' concept might have restricted features
representing the entire database of candidates for office and ways to
update that database, but only allow unrestricted access to the
database via a query (refined from the informal specification)
requesting the candidates for a particular race.

Formal concept specifications also have constraints, expressed as
\emph{preconditions} and \emph{postconditions} on features or as
\emph{invariants}, as appropriate. Preconditions, postconditions and
invariants are part of the Design by Contract development
technique~\cite{OOSC}: a precondition is a predicate that must be true
in order to invoke a feature; a postcondition is a predicate that must
be true after a feature has been invoked; and an invariant is a
predicate that must be true at all (observable) times. For example, an
informal constraint that the number of votes cast can never be
negative could be expressed as a postcondition \texttt{Result > 0} on
the \texttt{number\_of\_votes\_cast} feature (guaranteeing that the
result of invoking the feature is always non-negative), or could be
expressed as an invariant \texttt{number\_of\_votes\_cast > 0} on the
``election'' concept as a whole.

In addition to refinements of queries, commands and constraints, each
formal concept specification also contains refinements of inheritance
and client relationships described in the domain model. Inheritance
relationships are directly stated in formal concept specifications,
and client relationships are implicitly stated through the types
assigned to the concept's features.

A formal concept specification, like an informal one, has two main
purposes. First, it expresses some of the requirements of the system
in precise mathematical terms that can be checked for various
desirable properties, such as logical consistency, before
implementation begins; this allows for early detection of a class of
possible problems with the requirements or their realizations. Second,
it can be used with automated code generation tools to create a
significant amount of the implementation and source code specification
automatically, in a provably-correct and traceable fashion.

\subsection{Source Code Specifications}

A \emph{source code specification} is an annotation integrated into,
or otherwise associated with, a piece of source code that makes formal
statements about the code's behavior. Source code specifications are
typically direct refinements of the preconditions, postconditions and
invariants from formal concept specifications, and often are the same
mathematical statements rendered in a different syntax. However,
source code specifications may also formalize aspects of the system
that are not dealt with in the formal concept specifications, such as
memory and CPU usage limits, algorithmic efficiency, fine-grain
concurrency properties, and cross-concept properties that cannot be
expressed (or are difficult to express) in individual concept
specifications, but are otherwise expressed in, e.g., non-functional
requirements.

Source code specifications are generally written in a language that is
closely related to the implementation language, and in some cases are
written in the implementation language itself. Thus, the choice of
implementation language has a significant effect on what source code
specifications can be written and how much effort it takes to write
them. With appropriate tool support, which is available for several
implementation languages of various styles, many source code
specifications can be generated automatically from formal concept
specifications. Additionally, some can be inferred by analyzing parts
of the implementation.

Source code specifications enable extended static checking
(ESC)~\cite{ESC98} tools to verify, using automated theorem proving
techniques, that implementations satisfy their formal specifications
without actually executing the code. This verification is
\emph{modular}, meaning that individual components of the
implementation are verified in isolation. For example, when verifying
the postcondition of a feature $X$ the verifier makes a number of
assumptions: first, that $X$'s precondition is satisfied when $X$ is
invoked; second, that all invariants applicable to $X$ are satisfied
when $X$ is invoked; and third, that all features invoked by $X$
behave correctly with respect to their specifications. This modularity
allows static verification to be performed continuously during system
development in an efficient, incremental fashion, providing assurance
that the completed parts of the implementation are correct and
highlighting the parts of the implementation that do not yet satisfy
their specifications.

Source code specifications can provide significant benefits at runtime
as well. Runtime assertion checking compilers can use the
specifications to generate executable code that continuously checks
for runtime violations of the specification and flags errors when such
violations occur. Automated test generators can use the specifications
to generate high-coverage unit test suites to exercise the
implementation. Runtime assertion checking and automated test
generation can significantly increase the reliability of the finished
system, and in most cases require minimal developer effort after the
specifications are written.

\subsection{Protocol Specifications}

A \emph{protocol specification} is a formal description of an
information exchange among multiple parts of a system. Each distinct
type of information exchange in the system---such as registering a
candidate or casting a vote---has an associated protocol that must be
followed. We consider only \emph{application-level protocols} that
specify what type of information is exchanged, how it is encoded, how
(and whether) it is encrypted or digitally signed, and the sequences
of interactions the involved parties perform. We assume the existence
of well-specified lower-level protocols that enable information
transmission, such as the transport protocols used to encode
information into Internet Protocol packets and the physical protocols
used to convert those packets into electrical or optical signals and
send them to remote destinations.

Application-level protocols are described, typically in terms of
communicating finite-state machines, using one of several languages
specifically devised for protocol specification. Automated tools
process these descriptions to verify that the protocols have, or
demonstrate that they do not have, various properties. These include
both security properties, such as whether an adversary can gain access
to data that is supposed to be secret or modify data without being
detected, and non-security properties, such as whether the protocol
always terminates in an acceptable state. Protocol specification
languages and tools are typically designed to specify and verify
cryptographic protocols, but can also be used to specify and verify
insecure communication protocols by simply ignoring security
properties; in the case of an E2E-VIV system, the vast majority of the
application-level protocols are cryptographic protocols and require
security property verification.

The choice of protocol specification language is almost always
independent from any other specification language choices made when
engineering a system, because of its specialized nature: specifying
and verifying the interactions of a multiparty protocol is
significantly different from specifying and verifying the behavior of
individual features or software modules. However, once a protocol is
verified, parts of its formal description can be embedded in the
source code specifications of appropriate system modules. For example,
the module that implements the receiving side of a vote casting
protocol can (and should) contain an associated formal model of the
appropriate protocol state machine and its state transformation rules;
this model can then be verified and validated in a modular fashion
alongside the rest of the source code specifications to provide
evidence that the system actually implements the verified protocol.

\section{Implementation Methodology}

Once initial informal and formal specifications are developed, the
remaining stages of the software development process can begin. In
this section we describe some best practices for software
implementation, validation and maintenance that apply not only to
critical systems, but to software systems in general. These include
testing (the primary component of validation), version control and
continuous integration, issue tracking and code review, release
management, documentation practices, and process automation. This set
of practices is not mean to be exhaustive. For example, we do not
discuss particular code standard choices or the relative merits of
various Agile development methods, as these are primarily matters of
development team preference and have little bearing on the reliability
of the resulting software. Instead, we focus on practices that are
directly relevant to building and maintaining critical systems and
generating evidence of their correctness.

\subsection{Testing}

Software testing practices are a key component of any software
engineering methodology. Even when parts of a system are formally
verified, testing can provide additional assurance that the system
satisfies its requirements, behaves as expected for particular inputs,
works correctly in diverse environments, and has sufficient
performance.

Testing serves the key functions of uncovering flaws quickly and
ensuring that previously-fixed flaws do not recur in later software
versions. It is impossible for testing to reveal all possible flaws in
any realistic program---the number of possible inputs for any such
system is so large that an exhaustive test would effectively run
forever---but tests that successfully reveal and prevent recurrences
of some flaws provide some evidence for a system's correctness.

Verification and testing should not be viewed as opposing
alternatives, but rather as complementary techniques that together
provide assurance in the developed software. It is not feasible to
exhaustively test every possible input and every possible path through
any non-trivial program, while verification offers guarantees over all
possible inputs. However, verification usually cannot scale to provide
those guarantees for entire systems; verification tools must sometimes
make simplifying assumptions about the environment in which software
runs or reason about a simplified model of the actual system. Since
testing can exercise the real system in a real environment, it can
uncover flaws that are beyond the scope or capability of verification.

Different testing practices provide complementary types of assurance;
no single testing practice is sufficient. Instead, multiple types of
testing should be used in combination on every project.

\subsubsection{Unit Testing}

\emph{Unit testing} exercises the smallest components (the ``units'')
of a program that are feasible to test. The granularity of unit tests
varies by programming language, but typically unit tests are small
enough to test the implementation within a single module, class, or
other per-file abstraction.

Unit tests are usually written to reflect the specification of the
unit under test. For example, a unit that implements a specification
of addition might have unit tests that check the associativity and
commutativity of the implementation. Most software specifications are
too abstract to translate directly into unit tests so, for a property
like associativity, developers must choose particular concrete values
to test. Unfortunately, developers might not choose the particular
values that would expose a flaw; when they fail to do so, the test
suite will succeed despite the presence of that flaw.

Developer intuition and understanding of the implementation increases
the likelihood that unit tests will exercise code containing a flaw,
but tests still cannot be exhaustive. Code coverage, the percentage of
lines of code exercised by a given test suite, is often used to
measure the effectiveness of unit tests. In practice, high code
coverage percentages have not been shown to necessarily uncover more
flaws~\cite{inozemtseva2014coverage}, however more sophisticated
coverage measures such as branch coverage are more
promising~\cite{gligoric2013comparing}.

\subsubsection{Randomized and Fuzz Testing}

Manually-written tests can only exercise a small fraction of the
potential inputs to a program. When test cases can be generated
randomly, it is much cheaper to produce a large number of test cases
that can explore a larger fraction of potential inputs.

With \emph{randomized testing}, developers specify a means for
generating the inputs to a test, and provide a function or ``oracle''
for evaluating whether the test succeeded with those inputs. These
test generators can more closely mirror specifications than tests that
use concrete values, as they can instead make assertions about all
possible values.

For example, a unit test for an addition implementation might by hand
assert that $0+0=0$, $1+0=1$, and $2^{32}+0=2^{32}$, but a random test
could assert that for all integers $x$, $x+0=x$. Unless the range of
the input is very small, a random test will not provide an exhaustive
proof of the property it expresses. It can, however, easily provide
orders of magnitude more test cases than hand-written tests, and will
usually produce test cases that a developer might not think to add
intuitively. When a higher level of assurance is required, the
specification of a random test often translates directly to a logical
formula usable by formal techniques, easing the transition to a
verified system~\cite{swierstra2012xmonad}.

In the addition example above, it is straightforward to generate
random integers and check whether the results are correct. However,
randomized testing is situational, since an oracle is not always
straightforward to develop, and even the random input generator can be
quite complicated for complex input types. For example, random testing
has successfully been used to find flaws in C compilers. The
development of the C test program generator has itself been the
subject of extensive research~\cite{yang2011finding}, and the oracles
used are primarily other C compilers.

Randomized testing is difficult for programs with complex input types
and no readily-available oracles. However, a closely related technique
called \emph{fuzz testing} is useful for such programs. A fuzz testing
tool generates random input, perhaps guided by initial suggestions; it
then passes that input to the program under test and observes the
result. If the program crashes or violates any of its internal
assertions, the fuzz tester reports the result and the input that
caused it. If the program runs successfully, the fuzz tester tries
another random input. This process is repeated continuously until it
is stopped or, in some cases, until the fuzz tester determines that it
has fully exercised the program.

Fuzz testers use a variety of techniques to generate their test input,
including pure randomness, genetic algorithms (mutating previous
inputs to generate new ones)~\cite{DeMott07}, and symbolic execution
with constraint solving (calculating new inputs to avoid repeating
execution paths that have already been
tested)~\cite{Godefroid08}. Fuzz testing has exposed many critical
security issues in widely-used software, both open-source and
proprietary.

\subsubsection{Model-Based Testing}

Another method for automatically generating test cases is
\emph{model-based testing}, where formal models of the program's
behavior can be used as test oracles, as guides for choosing test
data, or both.

Formal models of program behavior can be used as test oracles by
compiling them into the software as \emph{runtime assertion
  checks}. For example, assume the specification for some function $f$
guarantees that some condition $x$ is true when it returns. If $x$ is
ever false when $f$ returns, $f$ does not satisfy its
specification. Any good set of unit tests for $f$ should detect this
divergence, and it would certainly be straightforward to write (by
hand) a test oracle that checked the value of $x$ after executing
$f$. However, in many cases $f$'s specification can be
\emph{automatically} transformed into a set of runtime assertions,
such that (in this case) an error is always raised if $x$ is false
when $f$ returns.  Compilers that perform such transformations are
available for several programming languages and corresponding
specification languages; some widely-used examples are Java and the
Java Modeling Language (JML), C\# and Code Contracts, C and the
Executable ANSI/ISO C Specification Language (E-ACSL), and Eiffel
(with its integrated specification language).

The set of runtime assertion checks derived from a formal
specification of a module effectively becomes a test oracle for that
module; in general, more precise specifications lead to better test
oracles. For a function $f$ with such an oracle, each possible input
to $f$ defines a test, and each test is run by simply calling $f$ with
that input. A test passes if all the runtime assertion checks pass and
fails if any runtime assertion check fails. If the specification of
$f$ restricts the set of possible inputs, tests that supply invalid
input are effectively meaningless and their output is ignored.

Of course, it is impractical to call $f$ with every possible input;
however, strategies such as randomized input data generation
(discussed above) and ``interesting'' input data generation (for
example, ensuring that all boundary conditions are tested for data
types with boundaries, such as numeric types) can be used to cover the
functionality of $f$ with a reasonable number of tests. Multiple test
frameworks use these techniques to automatically generate and run
model-based tests.

Formal models of program behavior can also be used as guides for
choosing test data; for example, some tools use constraint solving
techniques on programs and their specifications to ensure that test
data satisfies input constraints on the functions under test.
Techniques such as symbolic execution---analysis of the program to
determine what inputs cause what execution paths to be taken---can
also be used to choose test data, with the goal of achieving maximal
coverage using a minimal set of test cases. 

\subsubsection{Regression Testing}

In addition to unit and integration tests written to reflect the
specifications of a system, new tests should be added whenever a
defect is uncovered and fixed. Defects tend to recur in software for a
variety of reasons. The existence of the initial defect may imply that
there is some subtlety to that particular code, raising the baseline
likelihood of defects. The fix applied to the code may have only fixed
the defect under the limited set of conditions that were observed at
the time, for example in a bug report. The fix may also have depended
on assumptions about code elsewhere in the project, and the defect
might recur once those assumptions change.

Running a \emph{regression test} for every defect in the project's
history assures us that those defects are not present in the current
software artifacts. However, for a long project, the weight of that
history can make the regression suite unfeasibly large. Many
longer-term projects therefore split their regression tests into
multiple suites: a small, quick suite to run before each code commit,
a larger suite to run every night, and sometimes a full suite that
runs over weekends or before major project milestones. Since a goal of
testing is to uncover defects as quickly as possible, running tests
less frequently is a tradeoff, and prioritizing and minimizing the
cost of testing is an area of active
research~\cite{yoo2012regression}.

\subsubsection{Integration Testing}

While unit testing and regression testing find defects in individual
modules, \emph{integration testing} finds defects in the system as a
whole. This type of testing can expose flaws in the way multiple
modules interact, measure performance of the integrated system, reveal
environmental (e.g., configuration, operating system, network)
dependencies, and simulate the overall experience of the system's
users.

The most basic integration test is simply to check that the complete
system can be successfully built. Once built, integration tests
exercise substantial functionality across multiple modules, often
simulating the actions performed by a user during an interaction with
the system and checking for expected outcomes. In this sense,
integration tests are frequently the first line of validation applied
to a system.

For example, in an E2E-VIV system, one integration test might load a
ballot, make selections, change selections, and then cast the
ballot. Another integration test might follow the same steps, but then
spoil the ballot and repeat with a new ballot. While each of these
individual steps might concern only a single module, the combination
of steps helps expose potential problems with module interactions.

In addition to simulating overall functionality, integration tests
test the suitability of the software in its intended operating
environments. This is critical for systems that are intended to work
with multiple operating systems, with or without a network, or with
specialized hardware peripherals like a ballot marking device. Making
the environmental assumptions explicit in integration tests also helps
prevent defects from arising due to unstated assumptions. For example,
a developer may inadvertently write software that depends on features
that are unavailable in the deployed environment. It is
counterproductive and often infeasible to develop in the deployed
environment, which may not even be capable of running development
tools; therefore, the integration tests must accurately recreate that
environment.

\subsection{Version Control}

A \emph{version control system} (VCS) manages changes between the
versions of a project as it evolves during the course of
development. Revision control is the preferred way to share software
artifacts across a team, but all software projects, even those
developed by teams as small as one person should use a VCS.

In general, a developer uses the VCS first to ``check out'' the files
comprising a project into her ``working copy''. Then, after making
changes to those files, the developer ``checks in'' or ``commits'' the
changes to the VCS. After committing, those changes are available for
other developers to integrate into their working copies.

When different sets of changes have been made by multiple developers,
the VCS can merge those changes either automatically or by using
developer input to ensure the project remains consistent. The ability
to merge changes is critical for teams of developers who work
concurrently on a single project, and is the reason that file sharing
tools like Dropbox or Google Drive are an inadequate substitute for a
VCS.

Version control is particularly important for projects, such as
critical systems, where the development process must be auditable. An
entry to a project log is created every time a developer commits their
work. Any file in the project can be inspected to show its provenance,
even down to the level of which line was committed by which developer
on what date. Some VCS tools also allow for commits to be
cryptographically signed, offering assurance that, for example, the
changes have been audited by a trusted authority before being
integrated into the project~\cite{chacon2014pro}.

Moving from simple file storage to a VCS is a tremendous improvement
for development, but poor use of a VCS can negate many of the
potential benefits. For example, the log built from the commits of
developers is of less use to auditors if the changes in each commit
are not clearly associated with a particular new feature or bug
fix. Likewise, if developers commit changes that cause the system to
function incorrectly, other developers' productivity suffers and it
becomes more difficult later to isolate the commit that introduced a
bug. Each VCS supports multiple workflow practices that should be
adopted in order to limit these problems and get the most benefit from
VCS use~\cite{atlassianworkflow,pilato2008version}.

\subsection{Continuous Integration}

The expense of fixing software defects increases over time as other
parts of the software evolve around those defects. When a defect is
new, developers have not had a chance to write other code that depends
on the defective code. Once such dependencies exist, a fix for a
single defect can have consequences that ripple outward across the
entire project, making the fix much more expensive. The cheapest way
to fix a defect, then, is to discover it as quickly as possible.

\emph{Continuous integration} (CI) facilitates this by discovering
defects as part of a regular, automatic process that is not dependent
on due diligence of individual developers. CI interleaves the quality
control process into the development process, rather than leaving it
as a separate phase for the end of a project after development has
finished.

CI tools automatically build and test the latest version of the
software in the VCS system on a regular basis, such as every night or
after every VCS commit. Because the software is built from the VCS, it
is important for developers to frequently commit their work to the
VCS. The VCS integration ensures the tests are always run on the
canonical version of the software, and that any discovered defect can
be linked to a particular version in the VCS system. Isolating the
failing version focuses the efforts of developers on the set of
changes introduced in that version, often saving considerable time.

CI systems substantially replace manual effort and the risk of
mistakes when releasing software. Since the CI system builds the
project on a nightly basis, it can post the artifacts of those builds
for users and testers to quickly adopt. When an official release like
``Version 1.0'' is ready, developers can simply run the CI system to
produce the relevant artifacts. Because the same system is responsible
for both the continuous testing and validation of the system and the
creation of the final release, it is less likely that the final
release will have defects that would have been caught through earlier
testing.

\subsection{Configuration Management}

Complex software systems may depend on many components outside their
own implementations, including database servers, application servers,
web servers, authentication systems, etc. They may also need to run on
several different target platforms with different operating systems
and capabilities, or on multiple commercial cloud infrastructures with
different deployment and management processes.

\emph{Configuration management} is the tracking of dependencies and
deployment characteristics for a system, and is typically automated by
multiple tools. A \emph{build automation tool} handles dependency
tracking for building and packaging the software, and is used both by
developers and by the continuous integration system. A
\emph{deployment automation tool} handles the setup and configuration
of the (already built) software for a particular execution
environment, such as an operating system/platform combination or a
specific set of nodes at a specific cloud provider, including the
configuration of the individual machines on which the software will
run.

Configuration management saves significant manual effort and greatly
reduce the risks of mistakes when building and deploying software, and
should be used in all projects that have external dependencies or
target multiple deployment platforms.

\subsection{Software Product Lines}

Developing software that has to be changed in many ways, small and
large, for different target audiences, be they mobile phone models or
jurisdictions, has historically been a nightmare.  Each different
version of such a software is called a \emph{variant}, and the
decision-points in the software or its build system that derive
variants are called \emph{variation points}. This style of software
development is called \emph{software product lines}, or SPL for
short~\cite{czarnecki2005model,northrop2001software,clements2002software}.

Traditionally, one would either develop such software with programming
languages that have either (a)~support for conditional compilation,
usually in the form of C-like \texttt{\#ifdefs} or Lisp-like macros, or
(b) inheritance and use sub-typing and sub-classing as a means by which
to choose variants.  If one were to use process to effect the same
goal then typically a large tree of branches are created in a VCS,
where each branch is a mostly-separately maintained variant.

All of these styles of software development, without additional
supporting technology, have been shown to be unfit for this
purpose~\cite{linden2007software,kastner2008granularity,chen2009variability}.
Such software often ends up being an impenetrable ball of mud that is
extremely difficult to maintain.

The alternative way of developing SPLs is to use declarative means by
which to specify variants.  The academic literature highlights three
solid solutions to this problem: a specification technique called
\emph{feature models}~\cite{FMs}, using a specification language for
expressing variants in the world of model-driven development (MDD)
(e.g., Clafer or the OMG's Common Variability Language~\cite{haugen2012cvl}), or
embedded domain specific languages (EDSLs)~\cite{EDSL}.

Product support for feature modeling and CVL is in its infancy, but is
rapidly evolving as the industry demands better support for developing
SPL-based software systems~\cite{bosch2002maturity,bockle2004calculating}.

\subsection{Issue Tracking}

During much of the software development and maintenance process, teams
add features and fix bugs. An issue tracking system maintains records
about each new feature, each reported bug, and other discrete
development tasks from their creation to their implementation, review,
testing, and integration. Issues can be organized by metadata such as
assignee, project milestone, priority, and task type. Issue trackers
are essentially to-do lists with additional structure, specialized to
support effective software engineering.

These issues and their metadata give team members an overview of the
status and health of the project. For example, the issue tracker may
automatically require a code review step before an issue can be
resolved. Issue trackers help teams make fewer mistakes when following
best-practice software engineering workflows.

Team members can annotate issues with comments or attach supplementary
documents, creating a record of design decisions and thought
processes. This information is invaluable when investigating future
bugs or making subsequent changes to a design, and is often lost when
such discussions take place out-of-band and lose their associations
with the tasks that motivated them.

Most issue trackers integrate with VCS in order to associate issues
with source code changes. Combined with the design discussions
captured in issue comments and attachments, this enhances the ability
of the team to understand and maintain the project in the future.

When issue trackers are public they can also serve as a first point of
contact for users, providing insight into the evolution of the system
and a well-defined process for reporting system issues. In projects
with short development timelines, it is critical to incorporate
feedback into development as quickly as possible. Giving users or
front-line support staff the power to create issues directly makes the
feedback loop very small.

Public issue trackers also reduce duplicated effort by both users and
developers. If a system has a problem, that problem will likely become
apparent to multiple users; duplicate reports are less likely if users
can check the issue tracker for other reports of similar problems. The
development team can then triage issues by importance and urgency,
discuss potential solutions, assign developers to implement those
solutions, and finally make sure the problems are resolved and notify
the users who originally reported the problems.

\subsection{Code Review}

Code review practices involve examining the results of the software
development process to find defects, identify potential improvements,
and increase understanding of the software throughout the engineering
team. Reviews are also an opportunity to ensure that organizational
code style standards are met and that the code and its documentation
are easily understandable. This process, like discovering defects
during testing and investigating issue reports, feeds back into an
interative development process to improve the quality of the final
product.

Code review can be a manual process at varying levels of rigor. On the
formal end, processes like Fagan inspection require a line-by-line
inspection by many developers in an extended meeting and catch a high
percentage of defects~\cite{fagan2002design}. On the lightweight end,
code review occurs implicitly during pair programming and can take
place informally via a developer-led walkthrough or an email to
colleagues requesting feedback. Formal inspections are more costly
than informal reviews, but may be more suitable for projects that
require concrete audit trails for accountability. Lightweight methods,
particularly pair programming, can find similar proportions of defects
for lower cost~\cite{tomayko2002comparison} and have other positive
effects such as higher developer job satisfaction and improved team
dynamics~\cite{cockburn2000costs}.

Automated tools complement any form of manual code review. Lightweight
static analysis tools and code ``lint'' tools can help developers
avoid common coding mistakes and adhere to organizational style
standards. Very lightweight static tools can be run by individual
developers before committing their work to the VCS, and longer-running
analyses can be part of the continuous integration and testing
process. Such analyses are not substitutes for formal verification or
testing, however, as they typically are meant to discover small-scale
defects and help developers avoid common pitfalls rather than to
validate overall properties about the correctness of a system.

\subsection{Release Management and Lifecycle}

\emph{Release management} is the transformation of a set of software
artifacts into a finished product that can be used in its intended
environment. Release management is primarily focused on the smooth
integration of the different aspects of the project and on adhering to
practices that make releases repeatable, reliable, and auditable.

Release management and VCS workflows are tightly connected. For
example, in the Git Flow model~\cite{atlassianworkflow}, release
management would include creating a new release branch, imposing a
feature freeze (no new features, only bug fixes) on that branch, and
eventually tagging that branch upon release and merging it back into
the main development branch.

For a project that delivers software as a service on a web server,
release management would include deploying the software to production
servers. For software delivered as a binary download or CD, release
management would include cryptographically signing and distributing
the binary. In both of these cases, a release manager serves as the
final line of quality assurance before the software is used in its
intended environment, and must be fluent enough with all aspects of
the project and its processes to release software only once the
processes have been faithfully executed.

\subsection{Testable Documentation}

Documentation of the design, implementation, and use of a software
system is a standard requirement in software engineering
methodologies. However, when a system is under development and rapidly
changing, documentation can lag behind and fall out of step with the
latest version of the software, leading to errors and confusion.

Where possible, documentation should be machine-testable (or even
machine-generated) and integrated with the VCS rather than being a set
of static resources maintained independently of the software. Testable
and generated documentation is far less likely to become inconsistent with
the software it describes, because any such inconsistencies will be
detected during testing.

The form of testable documentation varies depending on the granularity
of the documentation and the underlying technologies used by the
project. For example, Business Object Notation
(BON)~\cite{walden1995seamless} can be used as analyzable
documentation at the specification, design, and architecture level.

At the level of code modules and interfaces, documentation should be
concretely executable like the ``doctest'' features available for
specification languages like JML (using its \texttt{examples} pragma)
and programming languages like Python and
Haskell~\cite{python3doctest}. Documentation in this style contains
short examples that illustrate the expected use of a system and its
expected response, for example in Python:

\begin{lstlisting}[language=Python]
"""
This is the fibonacci module. It provides the function fib which
returns the nth fibonacci number, where n >= 0.

>>> fib(0)
0
>>> fib(10)
55
>>> fib(-1)
Traceback (most recent call last):
    ...
ValueError: n must be >= 0
"""
\end{lstlisting}

Executable tests should supplement, not replace, traditional prose
documentation. Since they are essentially a form of unit test, they
suffer from the same limitations. They typically only exercise a
handful of concrete values, and cannot test non-functional properties
like expected performance or thread safety.

\subsection{Reproducibility and Automation}

A team can implement many of the techniques in this section
manually. Tests can be run by hand on developers' machines, code can
be sent out for review by email, issues can be tracked on a mailing
list, and a release manager can build and package release artifacts by
hand for each supported platform. However, each time a step in a
process must be manually performed, the probability of human error
increases and reproducing steps for later quality assurance and
troubleshooting becomes harder.

A manual operator might skip a step or perform a step out of order,
for example running the test suite before integrating the latest
changes from the VCS. The operator might also introduce new steps that
seem necessary and obvious, but unless recorded will make it very
difficult to reproduce or audit the process in the future. Finally,
manual execution of a process, even if done correctly, takes much
longer than automated execution.

To prevent errors, improve reproducibility, and make development more
efficient, processes should be automated as much as possible. The
techniques described in this section all support automation and
reproducibility or can themselves be automated to a degree, but some
play key roles.

\paragraph{Version Control}

The version control system is a linchpin of automation. The versions
it manages are the starting point for automated and reproducible
continuous integration, testing, and software releases. The VCS can
itself trigger automated processes; for example, it could trigger a
run of an automated test suite after every commit.

Any automated process should be run in the context of a particular VCS
version, and any artifacts produced by these processes should refer to
this version. For example, if software has a built-in bug reporting
feature, those reports should automatically include the version at the
time the software was built. This allows engineers to easily reproduce
the exact circumstances where a user discovered a defect.

Version control can only improve reproducibility when all relevant
inputs to a process are managed in the VCS. For example, a software
build process that depends on a configuration file in the user's home
directory would not be reproducible on a different computer without
that home directory.

\paragraph{Testing}

Manual testing can play an important role when evaluating a system,
but any realistic system requires more tests than are feasible to
perform manually. Even if the contents of a test suite are automatic,
if that test suite is only run manually it will often be skipped,
particularly when it takes a long time to run. Automating both the
tests themselves and the running of those tests ensures that they will
be run on a consistent basis, and that the results will be
reproducible and traceable to a particular version.

\paragraph{Continuous Integration}

Continuous integration is another linchpin of project
automation. Since CI tools are designed to automatically run on a
regular basis and offer integration with the VCS, other processes are
usually automated by using these tools. For example, after building
the integrated software, a CI tool should run the test suite and
archive the built artifacts for subsequent release management.

\paragraph{Configuration Management}

Configuration management is another area where automation is
critical. Tracking the software's dependencies and handling the
details of its deployment on various platforms should be left to
automated build and deployment automation tools to the extent
possible, and these tools should be run frequently. Typically,
configuration management tools are used by the continuous integration
system in addition to being used in release management.

\paragraph{Release Management}

No process has as many moving parts or cross-project concerns as
release management, making manual release management extremely
error-prone. The entire process, from checking out a version from the
VCS to deploying the final release artifacts, should be as automatic
as possible. The manual intervention should amount to simply deciding
which version to release and checking before the final release that
the automated process performed as expected.

Because automation is inexpensive when using continuous integration
and configuration management, it is a good practice to have those
tools perform parts of the release management process on a regular
basis, even when software is not ready for a release. If the process
of producing a release is the same as performing an ordinary nightly
build and test, it is less likely that problems will arise only at the
release stage when it is much more costly to address those problems.

\section{Technology Recommendations}
\label{sec:technologies}

Here, we provide some recommendations about specific technologies
that, at present, are well suited (and in some cases, \emph{not} well
suited) to performing rigorous software engineering of the type we
have described. These recommendations are based on experience applying
these methods over the last 15+ years, but they are not meant to be a
rigid set of rules or to unconditionally exclude technologies not
mentioned here. The landscape of software development languages and
tools is constantly changing; new languages and tools appear, while
old languages and tools disappear, are marginalized, or evolve in
possibly surprising ways.

\subsection{Domain Modeling}

For domain modeling, we recommend the Business Object Notation
(BON)~\cite{walden1995seamless} and Extended Business Object Notation
(EBON. BON is both a language and a design/refinement method
encompassing informal domain analysis and modeling, formal modeling,
and implementation-independent high- and medium-level
specification. BON has a well-defined semantics, is easy to learn and
write (especially the informal models, which are effectively
collections of simple English sentences), and has equally-expressive
textual and graphical notations. BON was originally developed for use
with the Eiffel programming language and is well-supported by the
EiffelStudio tool suite; however, it can be used with other
specification and implementation languages. EBON adds additional
\emph{semantic properties} to BON, allowing BON to express properties
relating to domains such as concurrency, ownership, responsibility,
bug tracking, literate programming, and version control.

We recommend BON over the Unified Modeling Language (UML), the de
facto standard for modeling in the software industry, for several
reasons. First, BON's equivalently expressive textual and graphical
notations are easy to work with and manipulate. UML supports only a
graphical notation, though there is also an unsupported official
``Human-Usable Textual Notation''~\cite{HUTN}; it was last updated in
2004, reflects only the version of UML that was current at the time,
and is not as expressive as the graphical notation. Tool support
currently exists for at least a dozen different and
mutually-incompatible textual UML dialects, none of which are as
expressive as the UML graphical notation and most of which have
significant readability issues.

Second, BON's semantics are an integral part of the language and
method and are easily understandable. By comparison, UML effectively
has no semantics; the Object Constraint Language (OCL)---the
specification language typically associated with UML models---is a
very complex expression language, and is not an integral part of UML.

Third, BON explicitly supports (and encourages) \emph{seamlessness}
and \emph{reversibility}. Seamlessness is the property that allows a
BON model to be smoothly (and, in many cases, completely
automatically) refined to lower-level specification languages, and
further to executable implementations. Reversibility is the property
that allows consistency to be maintained between the BON model, which
is an important part of the system documentation, and the resulting
implementation---when the implementation is changed, that change can
be (again, often completely automatically) propagated back up to the
BON model. Seamlessness and reversibility are both useful properties
for ensuring that the final software product accurately reflects the
original domain analysis and architecture design.

Fourth, BON supports high-level domain modeling using natural
language, making it easy to communicate models not only among software
developers but also with other stakeholders in the development
process. The BON representation of the E2E-VIV requirements in
\autoref{appendix:bon_requirements} consists almost entirely of simple
English sentences; it is therefore far more accessible to a wide
audience than an equivalent set of UML diagrams would be.

Finally, BON is \emph{simple}. Its specification is a fraction of the
size of the UML specification, and its graphical representation is
significantly less complex. For example, arrows in BON diagrams only
have two possible appearances (a single or double line, with a single
filled arrowhead) as compared to UML's proliferation of arrow types
(solid and dashed lines, filled and empty arrowheads, diamonds, and
circles, and additional connection markings). 

While we recommend BON based on our experience, it is not the only
reasonable choice for domain analysis in a rigorous software
engineering context. The Vienna Development Method (VDM)~\cite{VDM},
Z~\cite{Zed}, the B Method~\cite{BMethod}, and the Rigorous Approach
to Industrial Software Engineering (RAISE)~\cite{RAISE} are all
well-established domain analysis methods with textual representations,
well-defined semantics, and tool support.

\subsection{Formal Specification}

In addition to its use for domain modeling, BON can also be used as an
architecture specification and concept specification language and we
recommend its use as such. Once written, BON specifications can be
refined further into architectural specification languages for
expressing detailed architectural properties, source code
specification languages for integration with particular implementation
languages, and protocol specification languages for formalizing
interactions within the system.

UML is the most commonly used tool for architecture specification, but
is not well suited to rigorous software engineering because it lacks
semantics. We recommend using BON for high-level architecture
specification and a dedicated architecture specification language,
such as the SAE Architecture Analysis and Design Language
(AADL)~\cite{AADL} or the OMG Systems Modeling Language
(SysML)~\cite{SysML}, for low-level architecture
specification. Several tools support the creation and manipulation of
AADL and SysML specifications and automatic generation of code from
architectural models; some tools, like
OSATE2-Ocarina~\cite{OSATE2-Ocarina} for AADL, also support automated
verification.

Programming languages for which there is an obvious best choice of
source code specification language include Java
(JML~\cite{JMLReferenceManual}), C\# (Code
Contracts~\cite{CodeContracts}), and C (ACSL~\cite{ACSL}). Some
implementation languages, such as SPARK~\cite{SPARK2014} (a dialect of
Ada) and Eiffel, have integrated specification languages. These
specification languages all have several features that we consider
essential for efficient and effective use: straightforward syntax and
semantics, integration with widely-used software development
environments, and tool support for performing analysis and
verification. In the case of JML and Eiffel, tool support is also
available for automated reversible refinement from BON.

In any given project, different parts of the architecture may be
implemented in different languages. For example, it might be
appropriate to implement some computation-intensive parts of a design
in a language like C while implementing the rest of the design in a
language like Java or C\#. Thus, multiple source code specification
languages may be used in a single project, though we recommend that
high level specifications be written in a single language to the
extent possible.

There are also several language-neutral specification languages and
associated development tools, such as Alloy~\cite{Alloy},
Coq~\cite{Coq}, Event-B~\cite{Abrial10}, VDM~\cite{VDM}, and
Z~\cite{Zed}, which themselves support refinement to various
implementation languages. Most have associated tools for
\emph{model-based synthesis}, the process of automatically
transforming formal models into conforming executable
implementations. These languages and tools can be used effectively,
either by themselves or alongside other specification languages, in a
rigorous software engineering process; they are especially useful for
specifying, verifying, and automatically generating small, critical
system components.

Finally, any of several protocol specification languages can be used
to specify interaction protocols within the system. These include
the High-Level Protocol Specification Language (HLSPL) used by
Avispa~\cite{Avispa}, typed $\pi$-calculus as used by
ProVerif~\cite{ProVerif} and CryptoVerif~\cite{CryptoVerif},
Casper~\cite{Casper}, EasyCrypt~\cite{EasyCrypt}, and Scyther Protocol
Description Language (SPDL)~\cite{Scyther}. Each language is directly
tied to a protocol verification tool, and each tool has its own
strengths and weaknesses with respect to verifying different types of
protocol; thus, the choice of tool for verifying a given protocol
dictates the choice of language for specifying that protocol (or
vice-versa), and it may be appropriate to use different tools for
different protocols within the system.

\subsection{Implementation Language}

The choice of implementation language is clearly important, and there
are many possible choices; hundreds (possibly thousands) of
programming languages exist and dozens of those are actually viable,
with widespread adoption, tool support, and support communities. In
most cases, language choice determines programming style: for example,
Eiffel imposes an pure object-oriented style while Haskell imposes a
pure functional style. Language choice also determines the set of
existing functionality, in the form of standard libraries accompanying
the language or well-established external libraries available for use
with the language, on which developers can rely while building the
implementation. 

Implementation languages are distinguished by different styles,
different methods for error handling, different security guarantees,
and different ways in which programmers can make mistakes (both minor
and catastrophic). For rigorous software engineering, we generally
recommend languages with strong type systems that actively prevent
programmers from making any of a large class of errors. We also
recommend languages that automatically handle memory allocation and
deallocation, which prevents another large class of errors and many
potential security issues. Finally, we recommend languages that either
have good specification and verification tool support or are designed
explicitly for the implementation of high-assurance software. It is
certainly possible to implement reliable software in languages that do
not have all these features---for example, code can be written in a
safe subset of C, specified with ACSL, and verified with various
tools---but it is significantly more difficult to do so.

The nine implementation languages we currently recommend for rigorous
software engineering are (in alphabetical order) C (a safe subset such
as C$_{0}$~\cite{C0} or the C dialect supported by the Verified
Software Toolchain~\cite{VST}), C\# (excluding unsafe code), Eiffel,
Erlang, Gallina (the executable specification language for the Coq
proof assistant~\cite{Coq}), Haskell, Java, OCaml, and SPARK. No
single implementation language is ideal for every project, and it is
often appropriate to use multiple languages in the same project. For
example, it would be reasonable to write the computational core of a
voting system in a pure functional language like Haskell and the
voter-facing user interface components in an object-oriented language
like Java.

\subsection{Static Analysis}

Static analysis tools process the system's source code and
specifications to provide information about the system without
executing the code.  There are many forms of static analysis, ranging
from simple syntactic checks to full functional verification. The
following set of recommendations is not exhaustive, and in particular
does not discuss specific tools covering all the recommended types of
static analysis for all the recommended programming languages; many
useful static analysis tools that can be effectively deployed in a
rigorous software engineering process are not mentioned here.

At least one static analysis tool should be used to enforce some set
of code style and formatting guidelines, so that the implementation's
code base is consistently readable. Examples of tools that enforce
such guidelines are Checkstyle~\cite{Checkstyle} for Java and
StyleCop~\cite{StyleCop} for C\#. The style enforcement tool(s) should
be run automatically and frequently, either within each developer's
environment (for example, a Checkstyle plugin can run Checkstyle every
time a file is saved in any of the commonly-used Java IDEs) or as part
of the version control or continuous integration processes.

Static analysis tools should also be used to detect ``code smells''
and other problematic aspects of the implementation. The presence of
significant amounts of duplicate code in multiple locations in the
implementation and the excessive use of numeric literals that should
be declared as symbolic constants are examples of code smells; other
problematic aspects might (depending on the implementation language)
include memory leaks, buffer overflows, and concurrency problems.
Static analysis tools that can detect these issues include
FindBugs~\cite{FindBugs} and PMD~\cite{PMD} for Java,
ReSharper~\cite{ReSharper} for C\#, Eiffel
Inspector~\cite{EiffelInspector}, and SPARK Pro~\cite{SPARKPro}.

Finally, static analysis tools should be used to verify protocol
specifications and source code specifications. As previously
mentioned, the choice of tool for verifying protocol specifications is
dictated by the choice of protocol specification language. Source code
specifications can be verified with extended static checking tools,
such as OpenJML~\cite{OpenJML} for Java with JML specifications,
Frama-C~\cite{Frama-C} for C with ACSL specifications, and SPARK
Pro. In addition, source code can be proven equivalent to a reference
implementation or to a formal model using tools like the Software
Analysis Workbench (SAW)~\cite{SAW}; this type of analysis is
particularly useful when verifying the correctness of cryptographic
algorithms, which are pervasive in E2E-VIV systems.

\subsection{Dynamic Analysis}

Dynamic analysis tools monitor a running system to measure aspects of
its operation and detect undesirable behavior. There are many
different forms of dynamic analysis and many dynamic analysis tools;
like the static analysis recommendations above, the following set of
recommendations is not meant to be exhaustive.

One important form of dynamic analysis that we strongly recommend
using when possible is runtime assertion checking (RAC), previously
mentioned in associated with model-based testing. RAC compiles runtime
checks for specification conformance into the implementation, which
ensures that any runtime violations of the source code specifications
will be detected and reported. OpenJML for Java, Code Contracts for
C\#, SPARK 2014, and Eiffel all support RAC compilation with their
associated specification languages.

Another form of dynamic analysis that can be useful is
\emph{specification inference}. Tools like Daikon~\cite{Ernst2007},
which works on Java and C\# programs, can infer likely specifications
(particularly invariants) that may have been missed during the
specification refinement process. If these inferred specifications are
valid, and are added to the source code specifications, they can
assist ESC tools in verification and provide additional runtime
checks; in some cases, the additional specifications can allow ESC
tools to verify parts of the implementation that they otherwise could
not.

\emph{Coverage analysis} is another useful form of dynamic analysis,
particularly when used in conjunction with automated testing. Coverage
analysis tools, such as EMMA~\cite{EMMA} for Java,
OpenCover~\cite{OpenCover} for C\#, and
GNATcoverage~\cite{GNATcoverage} for SPARK, can provide information
about ``how much'' of the code was executed during a particular
run. ``How much'' can be expressed using several different metrics,
including \emph{statement coverage} (the percentage of the source
statements actually executed), \emph{branch coverage} (the percentage
of the program branches that were taken during the execution), and
\emph{path coverage} (the percentage of possible execution paths that
were taken during the execution). When used in conjunction with
automated testing these coverage metrics can give a rough idea of test
suite quality; for example, a test suite that does not exercise the
entirety of the system by at least one metric is clearly not as good
as a test suite that exercises the entirety of the system by all
coverage metrics. 

Dynamic analysis can also be used to detect issues related to memory
(leaks, corruption), concurrency (deadlock, spinning, data races),
resource allocation (unclosed sockets and files), and security (buffer
overflows and other vulnerabilities). Some of these issues are already
mitigated by the languages and other forms of analysis we
recommend---for example, none of the languages we recommend allow
programs to be vulnerable to buffer overflow attacks in the same way
that traditional C and C++ programs can be---but it is useful to run
dynamic analysis tools to detect the ones that are not. There are too
many such tools, of too many types, to list here.

Finally, dynamic analysis can be used to \emph{profile} the executable
code, monitoring it to determine which parts are executed most
frequently and to find performance bottlenecks. This allows developers
to gather empirical evidence for use in comparing multiple
implementation choices (e.g., what data structure variant to use for a
particular part of the system's data model), rather than blindly
guessing at the consequences of implementation decisions. It can also
help to direct optimization efforts when tuning the system for
performance late in the development process. All the languages we
recommend have associated profiling tools, many of which are
integrated into their respective development environments.

\subsection{Model Checking}

Model checkers attempt to determine whether a formal model of a
software system fulfills some specification, such as a requirement
that a concurrent system must not deadlock. If a model checker
determines that a model fulfills a specification, then we can conclude
that an implementation fulfills the specification if we can prove that
the implementation's behavior conforms to the model. Many of the
protocol verification tools mentioned previously use model checking
techniques.

The most widely-used model checking languages/tools are
Alloy~\cite{Alloy}, PVS~\cite{PVS}, Spin~\cite{Spin}, and
UPPAAL~\cite{UPPAAL}. Developers can write models in these languages
by hand, but it is more efficient to automatically \emph{extract}
models from existing implementation code; this guarantees that
properties proved about the model hold for the implementation. Model
extraction is supported for a number of implementation languages,

Model checking requires an exhaustive search of the model's reachable
states to determine whether any violate the specification. There are
several ways to reduce the complexity of this search, such as by
exploring multiple states with simultaneously in a symbolic fashion,
but model checking remains intractable for large software systems. We
therefore recommend that model checking be used sparingly, for only
the most critical subsystems, and that it be used primarily for
protocol verification.

\subsection{Version Control}

We recommend using either Git or Mercurial for version control. Both
are current-generation distributed version control systems, supporting
various models of collaboration, and both are well-supported. Both
also have associated services---including GitHub~\cite{GitHub} for
Git, BitBucket~\cite{BitBucket} for Mercurial, and
SourceForge~\cite{SourceForge} for either one---that provide
repository hosting, issue tracking, web hosting, and wiki
functionality. Git is currently the more popular of the two by a
significant margin, but both are good options for new projects and the
choice between them is mainly one of developer preference.

We recommend against using older, centralized version control systems
such as Subversion and CVS. In general, their mechanisms for handling
concurrent development and maintenance of multiple versions of
software are significantly more awkward than those of the
current-generation systems, and the single synchronization point
inherent in their centralized architectures makes it more difficult
for developers to work offline.

\subsection{Issue Tracking}

Many good issue tracking tools are available. If the version control
repository is hosted by one of the well-known hosting services
(GitHub, BitBucket, SourceForge), the obvious choice is to use the
issue tracker integrated with that service. Otherwise, there are
several proprietary and open-source choices for either cloud hosted or
locally installed issue tracking.

Atlassian's JIRA~\cite{JIRA}, JetBrains's YouTrack~\cite{YouTrack},
and Fog Creek Software's FogBugz~\cite{FogBugz} are all hosted issue
trackers that integrate with both Git and Mercurial repositories. JIRA
and YouTrack are also offered as standalone commercial products that
can be installed and maintained locally. All three have roughly
equivalent capabilities and the choice among them, like the choice
between Git and Mercurial, is largely a matter of developer
preference.

Trac~\cite{Trac} (along with its spinoff project Apache
BloodHound~\cite{ApacheBloodhound}) and Redmine~\cite{Redmine} are
open-source issue tracking systems supporting both Git and Mercurial,
which can be installed locally and used for free. They are also
reasonable choices when installed and managed appropriately, though
their user interfaces are generally less polished than those of the
commercial options.


\subsection{Continuous Integration and Configuration Management}

Using continuous integration and configuration management tools is
critical to a reliable software build process, and multiple good
options are available. Specific choices of build and deployment
automation tools are dependent on a project's implementation languages
and deployment scenarios; we list a selection of widely-used tools
here.

Jenkins~\cite{Jenkins} is a well-supported and
widely-used continuous automation tool; it is open source, supports
many languages and build automation tools, and integrates with many
development environments, issue trackers, and version control
systems. 

Some generic build automation tools are available, such as the
venerable GNU Make~\cite{GNUMake}, but most build automation tools
are developed to primarily support specific implementation
languages. For example, Apache Maven~\cite{ApacheMaven} and
Gradle~\cite{Gradle} primarily support Java development but can be
extended through plug-ins to support other implementation languages,
and Cabal~\cite{Cabal} supports Haskell development.

Several good choices exist for deployment
automation. Ansible~\cite{Ansible} and Puppet~\cite{Puppet} enable the
deployment of software across multiple machines running a variety of
operating systems, and also handle configuration of the execution
environments on those machines. Docker~\cite{Docker} takes a different
approach, automating the deployment of software inside virtualized
\emph{containers}; this effectively gives each deployed application
its own clean environment, preconfigured with all its dependencies,
without the overhead of creating and maintaining a large set of
physical or virtual machines. Unlike Ansible and Puppet, Docker only
supports deployment to Linux systems. 

\subsection{Testing}

Testing is an integral part of the development process, and many tools
exist to automate the generation of unit tests and the execution of
unit, regression and integration tests. In general we recommend that
tests be run as often and as non-interactively as possible, preferably
both as part of a continuous integration process and by individual
developers as they implement specific parts of the system.

Test automation frameworks are essential. Many unit test automation
frameworks take the same form, originated by the SUnit~\cite{SUnit}
framework for Smalltalk in the late 1990s, and are typically referred
to as \emph{xUnit} frameworks. Developers write (or generate) test
cases in a special format, combine them into test suites, and execute
them using a test execution program that gathers information about
which tests pass, which tests fail, and how any test failures
occur. The test execution program then presents this infromation to
the developer in a textual or graphical format.  JUnit~\cite{JUnit}
and TestNG~\cite{TestNG} for Java, NUnit~\cite{NUnit} for C\#, and
HUnit-Plus~\cite{HUnit-Plus} for Haskell are examples of such
frameworks. We strongly recommend the use of an xUnit framework for
test automation, especially for complex scenario tests that cannot be
automatically generated by other testing tools.

Randomized testing tools, such as the original
QuickCheck~\cite{QuickCheck} for Haskell and the many QuickCheck-like
tools developed for other languages (most of which are named
``QuickCheck for X'' or ``X-QuickCheck''), automatically generate
random unit tests. Many of these tools are guided in their test data
choices by performing constraint solving on existing source code
specifications; others require manual guidance on the part of the
developer. We recommend that randomized testing be used in most
projects, especially for automatic generation of simple unit tests
that would otherwise require significant developer effort.

Fuzz testing tools are particularly useful for exposing security
issues. These tools intentionally test invalid, unexpected, and random
data in an attempt to induce failures. Fuzz testing is primarily
applicable to software modules that directly process external input,
such as command line tools and Internet servers, and we strongly
recommend performing fuzz testing on all such modules in a
system. Fuzz testing tools with varying levels of speed and
effectiveness are available for most languages; the most prominent
example is AFLFuzz~\cite{AFLFuzz}, which was originally designed for C
but is runnable (at the cost of some efficiency) on binaries compiled
from any language. AFLFuzz is extremely effective and has revealed
bugs in many widely used software packages, including several critical
security issues.

Since all systems implemented using rigorous software engineering
techniques have at least some formal specifications, we also recommend
that some form of model-based testing be used if appropriate tools are
available for the implementation and specification languages. Several
test frameworks use model-based testing techniques to automatically
generate and run tests, including JMLUnitNG~\cite{ZimmermanNagmoti10}
for Java/JML, AutoTest~\cite{AutoTest10} for Eiffel, and
PEX~\cite{PEX08} for C\#.

\subsection{Roots of Trust}

All computing systems have multiple layers of abstraction; the lowest
layer is the hardware on which the system runs, followed by the
firmware, the operating system (which may itself have multiple
layers), and finally the application software. In general, higher
layers of the system must trust that lower layers are not malicious
and are performing according to their specifications. The \emph{roots
  of trust} in a computing system, also knoare the hardware and software
components of the system that are inherently trusted.

For example, in current general-purpose computers, the boot
firmware---the first code that executes when the machine is powered
on---is a root of trust. If the boot firmware is secure, the system
can use it to ``bootstrap'' a chain of trust; for example, the system
can use trusted cryptographic functions to verify the integrity of
software modules before loading them, and the loaded modules can then
be trusted to perform other functions. However, if the boot firmware
is compromised in some way, it can inject malicious code into any
software that it loads, and as a result none of the system can be
trusted. It is therefore critical to ensure that the boot firmware,
and any other roots of trust in a system, are protected from
tampering.

Currently, the only available way to ensure the integrity of a
system's roots of trust is by using a piece of dedicated hardware
called a Trusted Platform Module (TPM)~\cite{TPMSpec}. A TPM can check
the integrity of the device's boot firmware before allowing it to run,
and can also provide secure storage for encryption keys to protect the
contents of a machine's disk and authenticate authorized users before
allowing the machine to boot. By design, the integrity of a TPM can
only be compromised by a direct attack on its hardware such as
physical delamination of the TPM chip or direct measurement of its
radiation output. Thus, if the hardware has not been physically
tampered with, the TPM and the functionality embedded within it can be
considered secure and used to bootstrap trust at higher levels of
abstraction.

TPM functionality is embedded into many of today's computing
systems. However, the availability of TPM functionality is not enough;
the layers of software above a TPM must use it properly in order to
provide any assurance. Therefore, when building an E2E-VIV system, it
is essential that the roots of trust of the system be explicitly
enumerated and that the chain of trust for each originates in secure
hardware.

\section{Evidence-based Elections Technology}

Many of the technologies covered in this chapter produce, either as a
side-effect of their intended use or directly based upon their design,
machine-generated and machine-checkable evidence of systems
correctness or security.  Consequently, it seems sensible that federal
and state certification standards, particularly those issued by NIST
and adopted by the EAC, should at least mandate the production of such
evidence, and at most proscribe the use of specific processes,
methodologies, and technologies.  

Unfortunately, this is not the case.  Current and draft federal
standards, called the Voluntary Voting System Guidelines (VVSG), that
stipulate the means by which voting systems are tested and certified
for use in public elections mandate the use of no evidence-producing
techniques whatsoever.  Moreover, the certification gauntlet that
voting systems must survive, which focuses on process and checklists,
is based upon thirty year old thinking dismissed in both the academic
literature and in the industrial software engineering community. Any
system that is meant to be correct and secure developed using a
``quality process'' that produces a pile of paperwork is simply
unacceptable to researchers and developers in security and correctness
R\&D communities.

Based upon observations of the development methods practiced at
existing and past vendors, mainly via public audits of voting systems,
it also is clear that vendors do not use virtually all of the
concepts, tools, and techniques discussed in this chapter.  The
paucity of use of quality-centric software engineering and
evidence-generating technologies is not indicative of ignorance.  The
fact of the matter is that there is zero business pressure from
governments or the market, thus existing vendors have no reason to
do anything differently in the near future.

This state of affairs is radically different in other safety- and
mission-critical systems areas: software systems for aeronautics,
avionics, control systems, biomedical, nuclear energy, and spacecraft,
just to name a few, have stringent standards on the development and
certification of such software systems.  One might argue that public
election systems are no different, especially if they are to be used
for national public elections.

Discussions within and with NIST show that they are knowledgeable about
many of the concepts, tools, and technologies discussed in this
chapter and are quite keen on relying up more evidence-generating
techniques for voting system certification.  The question is, even if
VVSG evolves, and presuming that a future version of the VVSG covers
IV systems, how should third parties measure and assessment the
quality of E2E-VIV systems?  And, more importantly, how can and should
that evidence be interpreted by non-experts, particularly election
officials and the general public?

\subsection{Measuring and Assessing Quality}

There are numerous techniques covered in the peer-reviewed software
engineering literature for measuring and assessing the quality of a
software
system~\cite{meyer2003grand,gao2002testing,briand2000exploring,nagappan2008influence,kan2002metrics}.

Several existing standards, as mentioned above, have been discounted
as directly contributing to software quality, and thus should not be
used in the assessment of such for E2E-VIV system.  These include
ISO-9000, CMMI, IEC 62304, and FIPS
140-2~\cite{ISO9000,herbsleb1997software,IEC-62304,FIPS-140-2}.

Several current and past international standards---namely the Common
Criteria Certification~\cite{ISO-IEC-15408-1,ISO-IEC-15408-2,ISO-IEC-15408-3}, Future Airborne Capability
Environment (FACE)~\cite{FACE}, Avionics Application Standard Software
Interface (ARINC) 653~\cite{ARINC653}, and the Software Considerations
in Airborne Systems and Equipment Certification (DO-178B and
DO-178C)~\cite{DO178B,DO178C} among others---do directly address the
necessary theoretical and technical foundations for what constitutes a
quality software system.  As such, we recommend that these existing
standards should be practically evolved and adopted by NIST in future
versions of the VVSG.

Two certification frameworks, one general-purpose and one exemplar,
provide a reasonable ground for future voting systems certification
standards.

The general framework is the NIST risk management framework (NIST SP
800-37 and NIST SP 800-53~\cite{NIST-800-37,NIST-800-53}).  It
constitutes an excellent high-level characterization of the means by
which the risks to secure system should be evaluated and mitigated.

Exemplars include standards from the Payment Cards Industry Security
Standards Council, such as their PCI Data Security Standard (PCI
DSS)~\cite{PCI-DSS}.  The PCI DSS, much like the highest levels of
FIPS 140-2, helps developers carefully specify exactly the trusted
code base of mission-critical systems; a fundamental exercise in the
context of E2E-VIV systems.

In conclusion, independent of the evolution of the VVSG, the evaluation
of the quality of any E2E-VIV system, as mandated by one of the
recommendations in the concluding chapter of this report, should
principly reside upon objective machine-interpretable evidence.

\subsection{Interpreting Evidence for the Layperson}

Today the layperson must delegate trust in the quality of their voting
equipment.  The voter delegates their trust to their election
official.  Election officials delegate trust to their certification
authority at the state or federal level.  Certification authorities
delegate their trust to one of three accredited test
laboratories~\cite{VVSG-labs}.  Test laboratories, which are hired by
vendors to certify their products, certify a specific voting system
release against a specific VVSG version. In the main this means that
the lab manually evaluates the voting system against an enormous
checklist and produces reams of documentation, but little-to-no
third-party checkable evidence of the system's correctness and
security.

This chain of delegated trust that eventually rests in a paid test
laboratory that does not produce publicly digestible or publicly
checkable evidence is clearly not sufficient for the certification of
E2E-VIV systems.

In order to give verifiable evidence of an E2E-VIV system's
correctness and security to the layperson we must continue to rely
upon delegated trust.  But, critically, \emph{the roots of that trust
  must be up to the layperson}.

In the E2E-VIV setting, this transparent, multi-party delegated trust
is contingent upon the definition and realization of several
requirements found in this report.  The key requirements mandated
herein are that any E2E-VIV system must:
\begin{enumerate}
\item have a formally specified, machine-checked, E2E protocol,
\item use only publicly documented, and preferably standards-based,
  data file and wire protocol formats,
\item produce third-party verifiable artifacts (such as ZKPs,
  independently-checkable hashchains, etc.),
\item include a comprehensive set of traceable automated functional
  and non-functional tests, and
\item have formal, preferably mechanically-checked, proofs of the
  correctness and security of the E2E protocol and its realization.
\end{enumerate}

By mandating the these evidence-generating requirements the trust in
an E2E-VIV system need not be delegated to subjectively untrusted
individuals or organizations. Any organization or individual with
either the requisite skills (say, a computer science Ph.D. or a
talented software engineer) or reasonable resources (with which to
hire a group of trusted technical experts) can now objectively
evaluate the quality of an E2E-VIV system.  

Given the vast political variety of individual experts, think tanks,
IT corporations, finding those with the requisite skills should be
straightforward.  

Moreover, given the wealth of well-organized, sometimes enormously
well-funded, entities whose focus is entirely on public elections, it
is reasonable to expect that third-party certification of E2E-VIV
systems is tractable.  Hypothetically, only a small set of key
organizations---say the DNC, RNC, a small set of independent top
computer scientists and cryptographers involved with the Elections
Verification Network or the Verified Voting Foundation, and a PAC
aligned with each candidate---is necessary to earn the trust of
virtually every voter in America.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "report"
%%% End:
